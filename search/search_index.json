{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction to McLaren Synth Kit The McLaren Synth Kit aims to bring audio programming to GNUstep Objective-C on Linux. It consists of a number of parts. Alsa Sound Kit (ASK) - Obj-C wrappers around Alsa sound and MIDI devices McLaren Synth Kit (MSK) - a more abstract sound \"context\" and synthesizer construction operators like oscillators, envelopes and effects. Demonstration programs and applications. Project Home The documentation you are reading here is from the /mkdocs directory of the McLaren Synth Kit repository on github. https://github.com/mclarenlabs/libs-mclaren-alpha Background This project is partially inspired by experiences with audio systems on IOS, OSX and Webkit and a NeXT project called MusicKit . We had in mind building a networked synthesizer using RTP-MIDI in an embedded Raspberry Pi application. We really liked the features of modern Objective-C and thought it could provide a nice experience for audio programming. We purposely chose to focus on Linux and its ALSA interface. We wanted to avoid the layers of abstraction provided by the PortAudio project or JUCE. We also wanted to avoid dependence on a sound server like Pulse or JACK simply to reduce dependencies. We wanted to keep the toolkit lean and easy to set up. These decisions helped make the toolkit simple to use for simple things. Along the way we learned a lot and discovered some interesting capabilities. Hopefully this project will help share the things we learned. Concurrency One of the things that makes audio programming difficult is the management of concurrency. In an audio application like a synthesizer, there is usually a thread dedicated to rendering the audio, a thread dedicated to MIDI, a thread managing the GUI, and possibly other threads performing background tasks. Apple's GCD (Grand Central Dispatch) aka \"dispatch queues\" help simplify concurrency, and they are a first-class part of GNUstep's Objective-C environment. Instead of writing code that manages threads and locks, your code is organized into blocks that execute sequentially in a dispatch queue concurrency domain. The syntactic sugar of \"blocks\" in C lets you write code that executes in one concurrency domain and launches an operation in another. This replaces a cross-thread operation with something simpler. One of the design goals of the McLaren Synth Kit it to elevate audio programming to the realm of dispatch events, and to hide threads and locks as much as possible. We think we have succeeded. Memory Management The McLaren Synth Kit is designed to take full advantage of ARC (Automatic Reference Counting) for memory reclamation. A synthesizer with polyphony is usually rendering many different sounds at once. Each sound has a starting event (its \"attack\") and eventually its \"decay\" and \"release.\" It's convenient to think of sounds as being allocated and deallocated. While they are allocated, they are rendered to produce a sound. With the Synth Kit, a sound unit (called a \"Voice\") is dynamically allocated and configured, and then is handed to the audio thread for rendering. After the Voice is done playing, the audio thread marks it a reclaimable. A different background thread periodically checks for reclaimable Voices and gives them back to ARC. This puts the memory of the Voice back in the allocation pool. These details are handled by the Synth Kit. When you're writing an audio program using the kit, you only need to allocate and configure voices. Once you're done with a sound, Synth Kit takes care of cleaning up. Objective-C is C Objective-C is strictly a superset of C. This means that C structs and functions can be used directly in Objective-C programs. ALSA makes use of opaque C pointers as well as C structs for defining events and messages. These things are the \"language\" of ALSA. The McLaren Synth Kit exposes many of these details where it adds no value to abstract them. Objective-C objects can be referenced as C structs. This capability can lead to dangerous programming territory, but when used carefully it is very powerful. In the McLaren Synth Kit, we take advantage of the memory management and property capabilities of Objective-C objects, but we can (safely) pass references to these structs to low-level C functions. C functions can access Objective-C instance variables directly, because the memory layout is known by both C and Objective-C. Error Handling We tried to normalize errors that arise in a number of contexts. Wherever possible, errors exposed as NSError objects. Initializers that may encounter an error have an extra error:(NSError**) argument in which to describe the problem. StepTalk It's interesting to think about how an interpreted language could be used to build audio interfaces and stitch together audio units to create user-defined synthesize instruments. StepTalk is a GNUstep project that implements a Smalltalk-like interpreted language on top of the Objective-C runtime environment. We have only just begun to build demonstration programs with StepTalk, but you can see an example of how this could work in the application MidiScriptDemo in the Applications/ directory. A Brief Tour of the Components The tool kit is divided into two layers. The Alsa Sound Kit (ASK) provides minimal Objective-C wrappers around ALSA sound and MIDI sequencer devices. It also encapsulates standard algorithms on these things, so that instead of writing code to play a sound or enumerate the sequencers in your system, you call a method or set up a block. The major components of the Alsa Sound Kit are described below. ASKPcm - open an ALSA audio device and play or capture sounds ASKPcmList - list the ALSA audio devices in your system ASKSeq - open an ALSA MIDI Sequencer device and play or capture MIDI events ASKSeqList - list the ALSA MIDI devices in your system The McLaren Synth Kit (MSK) further abstracts audio devices into an object called a \"Audio Context.\" An Audio Context can render audio pipelines that are described as a connected graph of audio elements, each of which is called a \"Voice\". A Voice can be a simple oscillator, an oscillator controlled by an envelope, or a more complex graph of multiple oscillators, envelopes and filters. A Voice can play for ever, it can stop after a specified time or it can be sent a noteOff message. Once created, a Voice is handed over to a Context for rendering. The Context manages the rendering and mixing of multiple Voices. In a typical use, a Voice is allocated for each MIDI note sounding. When a Voice is done playing, the Context removes the Voice and arranges for the reclamation of its memory. The major components of the McLaren Synth Kit are described below. MSKContext - an audio context for rendering Voices or capturing sounds into a Voice MSKLinEnvelope, MSKExpEnvelope - linear and exponential envelope generators MSKSinFixedOscillator - a simple sinusoidal oscillator MSKGeneralOscillator - Sin, Squarewave, Triangle, Saw oscillator MSKPhaseDistortionOscillator - an oscillator who phase is controlled by another Voice The parameters of the various kinds of voices are continuously variable. To allow multiple voices to share configuration parameters (for example, all of the notes playing in a MIDI channel to have the same \"transpose\" or \"pitchbend\" setting) the continously variable parameters of voices are separated into objects called Models. The models in the system are listed below. MSKEnvelopeModel MSKOscillatorModel MSKReverbModel Aside: Models have a special role in the system: they provide read-only \"C\" variables that the Voices read from the audio thread. And for each \"C\" variable, they expose an Objective-C \"property\" for reading and writing that variable. To the outside-world, their properties can be freely used with Cocoa controls and Cocoa bindings. The audio thread reads the primitive \"C\" values behind the models to respond to value changes. A Complete Example Shown below is a complete example of a program that opens a device and plays a sound using a MSKContext . The top of the program opens a new Audio Context, configures it and starts it running. Dispatch queue calls are used to schedule events. At time t=1 second, a block is launched that creates a new sound and adds it to the context. This starts the sound playing until its release time has passed. At time t=2 seconds, a block is launched that closes the context. Lastly at time t=4 seconds a block is launched to exit the program. This program is available in the msk-examples directory and is named tiny.m . You can compile it yourself and try it out. int main(int argc, char *argv[]) { // Desired audio context parameters MSKContextRequest *request = [[MSKContextRequest alloc] init]; request.rate = 44000; request.persize = 1024; request.periods = 2; NSString *devName = @\"default\"; NSError *error; BOOL ok; // Create an audio context on the 'default' device for playback MSKContext *ctx = [[MSKContext alloc] initWithName:devName andStream:SND_PCM_STREAM_PLAYBACK error:&error]; if (error != nil) { NSLog(@\"MSKContext init error:%@\", error); exit(1); } // Configure the context with the request ok = [ctx configureForRequest:request error:&error]; if (ok == NO) { NSLog(@\"MSKContext configure error:%@\", error); exit(1); } // Start the context ok = [ctx startWithError:&error]; if (ok == NO) { NSLog(@\"MSKContext starting error:%@\", error); exit(1); } // Create and sound a Voice after 1 second dispatch_time_t attackt = dispatch_time(DISPATCH_TIME_NOW, 1.0*NSEC_PER_SEC); dispatch_after(attackt, dispatch_get_main_queue(), ^{ MSKOscillatorModel *oscmodel1 = [[MSKOscillatorModel alloc] initWithName:@\"osc1\"]; oscmodel1.osctype = @MSK_OSCILLATOR_TYPE_TRIANGLE; MSKEnvelopeModel *envmodel1 = [[MSKEnvelopeModel alloc] initWithName:@\"env1\"]; envmodel1.sustain = 1.0; envmodel1.rel = 1.5; MSKExpEnvelope *env = [[MSKExpEnvelope alloc] initWithCtx:ctx]; env.oneshot = YES; env.shottime = 0.05; env.model = envmodel1; [env compile]; MSKGeneralOscillator *osc = [[MSKGeneralOscillator alloc] initWithCtx:ctx]; osc.iNote = 60; osc.sEnvelope = env; osc.model = oscmodel1; [osc compile]; // Hand the oscillator over to the context for rendering [ctx addVoice:osc]; }); // Close the context after two seconds dispatch_time_t after = dispatch_time(DISPATCH_TIME_NOW, 2.0*NSEC_PER_SEC); dispatch_after(after, dispatch_get_main_queue(), ^{ [ctx close]; }); // Exit after four seconds dispatch_time_t after4 = dispatch_time(DISPATCH_TIME_NOW, 4.0*NSEC_PER_SEC); dispatch_after(after4, dispatch_get_main_queue(), ^{ exit(0); }); // Run forever [[NSRunLoop currentRunLoop] run]; } Summary This chapter gives a brief introduction to the design goals of the McLaren Synth Kit. The kit uses the concurrency and memory management features of modern Objective-C to provide a comfortable environment for audio programming on Linux. An example program showed how to play a synthesized sound consisting of an envelope generator and an oscillator playing a middle-C. Hopefully this was intriguing enough that you want to find out more. We didn't explain the details of how audio objects are defined and configured using the McLaren Synth Kit, and we didn't explain the rules for how \"Voices\" (the audio objects of the MSK) can be combined to create more complex sounds. We also didn't cover how to find audio devices on your computer (i.e. - external USB sound cards) and how to use them. All of these will be covered in later chapters.","title":"Introduction to McLaren Synth Kit"},{"location":"#introduction-to-mclaren-synth-kit","text":"The McLaren Synth Kit aims to bring audio programming to GNUstep Objective-C on Linux. It consists of a number of parts. Alsa Sound Kit (ASK) - Obj-C wrappers around Alsa sound and MIDI devices McLaren Synth Kit (MSK) - a more abstract sound \"context\" and synthesizer construction operators like oscillators, envelopes and effects. Demonstration programs and applications.","title":"Introduction to McLaren Synth Kit"},{"location":"#project-home","text":"The documentation you are reading here is from the /mkdocs directory of the McLaren Synth Kit repository on github. https://github.com/mclarenlabs/libs-mclaren-alpha","title":"Project Home"},{"location":"#background","text":"This project is partially inspired by experiences with audio systems on IOS, OSX and Webkit and a NeXT project called MusicKit . We had in mind building a networked synthesizer using RTP-MIDI in an embedded Raspberry Pi application. We really liked the features of modern Objective-C and thought it could provide a nice experience for audio programming. We purposely chose to focus on Linux and its ALSA interface. We wanted to avoid the layers of abstraction provided by the PortAudio project or JUCE. We also wanted to avoid dependence on a sound server like Pulse or JACK simply to reduce dependencies. We wanted to keep the toolkit lean and easy to set up. These decisions helped make the toolkit simple to use for simple things. Along the way we learned a lot and discovered some interesting capabilities. Hopefully this project will help share the things we learned.","title":"Background"},{"location":"#concurrency","text":"One of the things that makes audio programming difficult is the management of concurrency. In an audio application like a synthesizer, there is usually a thread dedicated to rendering the audio, a thread dedicated to MIDI, a thread managing the GUI, and possibly other threads performing background tasks. Apple's GCD (Grand Central Dispatch) aka \"dispatch queues\" help simplify concurrency, and they are a first-class part of GNUstep's Objective-C environment. Instead of writing code that manages threads and locks, your code is organized into blocks that execute sequentially in a dispatch queue concurrency domain. The syntactic sugar of \"blocks\" in C lets you write code that executes in one concurrency domain and launches an operation in another. This replaces a cross-thread operation with something simpler. One of the design goals of the McLaren Synth Kit it to elevate audio programming to the realm of dispatch events, and to hide threads and locks as much as possible. We think we have succeeded.","title":"Concurrency"},{"location":"#memory-management","text":"The McLaren Synth Kit is designed to take full advantage of ARC (Automatic Reference Counting) for memory reclamation. A synthesizer with polyphony is usually rendering many different sounds at once. Each sound has a starting event (its \"attack\") and eventually its \"decay\" and \"release.\" It's convenient to think of sounds as being allocated and deallocated. While they are allocated, they are rendered to produce a sound. With the Synth Kit, a sound unit (called a \"Voice\") is dynamically allocated and configured, and then is handed to the audio thread for rendering. After the Voice is done playing, the audio thread marks it a reclaimable. A different background thread periodically checks for reclaimable Voices and gives them back to ARC. This puts the memory of the Voice back in the allocation pool. These details are handled by the Synth Kit. When you're writing an audio program using the kit, you only need to allocate and configure voices. Once you're done with a sound, Synth Kit takes care of cleaning up.","title":"Memory Management"},{"location":"#objective-c-is-c","text":"Objective-C is strictly a superset of C. This means that C structs and functions can be used directly in Objective-C programs. ALSA makes use of opaque C pointers as well as C structs for defining events and messages. These things are the \"language\" of ALSA. The McLaren Synth Kit exposes many of these details where it adds no value to abstract them. Objective-C objects can be referenced as C structs. This capability can lead to dangerous programming territory, but when used carefully it is very powerful. In the McLaren Synth Kit, we take advantage of the memory management and property capabilities of Objective-C objects, but we can (safely) pass references to these structs to low-level C functions. C functions can access Objective-C instance variables directly, because the memory layout is known by both C and Objective-C.","title":"Objective-C is C"},{"location":"#error-handling","text":"We tried to normalize errors that arise in a number of contexts. Wherever possible, errors exposed as NSError objects. Initializers that may encounter an error have an extra error:(NSError**) argument in which to describe the problem.","title":"Error Handling"},{"location":"#steptalk","text":"It's interesting to think about how an interpreted language could be used to build audio interfaces and stitch together audio units to create user-defined synthesize instruments. StepTalk is a GNUstep project that implements a Smalltalk-like interpreted language on top of the Objective-C runtime environment. We have only just begun to build demonstration programs with StepTalk, but you can see an example of how this could work in the application MidiScriptDemo in the Applications/ directory.","title":"StepTalk"},{"location":"#a-brief-tour-of-the-components","text":"The tool kit is divided into two layers. The Alsa Sound Kit (ASK) provides minimal Objective-C wrappers around ALSA sound and MIDI sequencer devices. It also encapsulates standard algorithms on these things, so that instead of writing code to play a sound or enumerate the sequencers in your system, you call a method or set up a block. The major components of the Alsa Sound Kit are described below. ASKPcm - open an ALSA audio device and play or capture sounds ASKPcmList - list the ALSA audio devices in your system ASKSeq - open an ALSA MIDI Sequencer device and play or capture MIDI events ASKSeqList - list the ALSA MIDI devices in your system The McLaren Synth Kit (MSK) further abstracts audio devices into an object called a \"Audio Context.\" An Audio Context can render audio pipelines that are described as a connected graph of audio elements, each of which is called a \"Voice\". A Voice can be a simple oscillator, an oscillator controlled by an envelope, or a more complex graph of multiple oscillators, envelopes and filters. A Voice can play for ever, it can stop after a specified time or it can be sent a noteOff message. Once created, a Voice is handed over to a Context for rendering. The Context manages the rendering and mixing of multiple Voices. In a typical use, a Voice is allocated for each MIDI note sounding. When a Voice is done playing, the Context removes the Voice and arranges for the reclamation of its memory. The major components of the McLaren Synth Kit are described below. MSKContext - an audio context for rendering Voices or capturing sounds into a Voice MSKLinEnvelope, MSKExpEnvelope - linear and exponential envelope generators MSKSinFixedOscillator - a simple sinusoidal oscillator MSKGeneralOscillator - Sin, Squarewave, Triangle, Saw oscillator MSKPhaseDistortionOscillator - an oscillator who phase is controlled by another Voice The parameters of the various kinds of voices are continuously variable. To allow multiple voices to share configuration parameters (for example, all of the notes playing in a MIDI channel to have the same \"transpose\" or \"pitchbend\" setting) the continously variable parameters of voices are separated into objects called Models. The models in the system are listed below. MSKEnvelopeModel MSKOscillatorModel MSKReverbModel Aside: Models have a special role in the system: they provide read-only \"C\" variables that the Voices read from the audio thread. And for each \"C\" variable, they expose an Objective-C \"property\" for reading and writing that variable. To the outside-world, their properties can be freely used with Cocoa controls and Cocoa bindings. The audio thread reads the primitive \"C\" values behind the models to respond to value changes.","title":"A Brief Tour of the Components"},{"location":"#a-complete-example","text":"Shown below is a complete example of a program that opens a device and plays a sound using a MSKContext . The top of the program opens a new Audio Context, configures it and starts it running. Dispatch queue calls are used to schedule events. At time t=1 second, a block is launched that creates a new sound and adds it to the context. This starts the sound playing until its release time has passed. At time t=2 seconds, a block is launched that closes the context. Lastly at time t=4 seconds a block is launched to exit the program. This program is available in the msk-examples directory and is named tiny.m . You can compile it yourself and try it out. int main(int argc, char *argv[]) { // Desired audio context parameters MSKContextRequest *request = [[MSKContextRequest alloc] init]; request.rate = 44000; request.persize = 1024; request.periods = 2; NSString *devName = @\"default\"; NSError *error; BOOL ok; // Create an audio context on the 'default' device for playback MSKContext *ctx = [[MSKContext alloc] initWithName:devName andStream:SND_PCM_STREAM_PLAYBACK error:&error]; if (error != nil) { NSLog(@\"MSKContext init error:%@\", error); exit(1); } // Configure the context with the request ok = [ctx configureForRequest:request error:&error]; if (ok == NO) { NSLog(@\"MSKContext configure error:%@\", error); exit(1); } // Start the context ok = [ctx startWithError:&error]; if (ok == NO) { NSLog(@\"MSKContext starting error:%@\", error); exit(1); } // Create and sound a Voice after 1 second dispatch_time_t attackt = dispatch_time(DISPATCH_TIME_NOW, 1.0*NSEC_PER_SEC); dispatch_after(attackt, dispatch_get_main_queue(), ^{ MSKOscillatorModel *oscmodel1 = [[MSKOscillatorModel alloc] initWithName:@\"osc1\"]; oscmodel1.osctype = @MSK_OSCILLATOR_TYPE_TRIANGLE; MSKEnvelopeModel *envmodel1 = [[MSKEnvelopeModel alloc] initWithName:@\"env1\"]; envmodel1.sustain = 1.0; envmodel1.rel = 1.5; MSKExpEnvelope *env = [[MSKExpEnvelope alloc] initWithCtx:ctx]; env.oneshot = YES; env.shottime = 0.05; env.model = envmodel1; [env compile]; MSKGeneralOscillator *osc = [[MSKGeneralOscillator alloc] initWithCtx:ctx]; osc.iNote = 60; osc.sEnvelope = env; osc.model = oscmodel1; [osc compile]; // Hand the oscillator over to the context for rendering [ctx addVoice:osc]; }); // Close the context after two seconds dispatch_time_t after = dispatch_time(DISPATCH_TIME_NOW, 2.0*NSEC_PER_SEC); dispatch_after(after, dispatch_get_main_queue(), ^{ [ctx close]; }); // Exit after four seconds dispatch_time_t after4 = dispatch_time(DISPATCH_TIME_NOW, 4.0*NSEC_PER_SEC); dispatch_after(after4, dispatch_get_main_queue(), ^{ exit(0); }); // Run forever [[NSRunLoop currentRunLoop] run]; }","title":"A Complete Example"},{"location":"#summary","text":"This chapter gives a brief introduction to the design goals of the McLaren Synth Kit. The kit uses the concurrency and memory management features of modern Objective-C to provide a comfortable environment for audio programming on Linux. An example program showed how to play a synthesized sound consisting of an envelope generator and an oscillator playing a middle-C. Hopefully this was intriguing enough that you want to find out more. We didn't explain the details of how audio objects are defined and configured using the McLaren Synth Kit, and we didn't explain the rules for how \"Voices\" (the audio objects of the MSK) can be combined to create more complex sounds. We also didn't cover how to find audio devices on your computer (i.e. - external USB sound cards) and how to use them. All of these will be covered in later chapters.","title":"Summary"},{"location":"XXintro/","text":"Introduction to the Mclaren Synth Kit The McLaren Synth Kit aims to bring audio programming to Objective-C on Linux. It consists of two parts. Alsa Sound Kit (ASK) - Obj-C wrappers around Alsa sound and MIDI devices Mclaren Synth Kit (MSK) - a more abstract sound \"context\" and synthesizer construction operators like oscillators, envelopes and effects. These tool kits are distributed as header files and static libraries compiled for specific compiler and runtime combinations. FYI: These are the same libraries McLaren Labs uses to construct the synthesizers that are available for free download at https://mclarenlabs.com . Project Home The documentation you are reading here is from the /mkdocs directory of the McLaren Synth Kit repository on github. https://github.com/mclarenlabs/McLarenSynthKit Background This project is partially inspired by experiences with audio systems on IOS, OSX and Webkit and a NeXT project called MusicKit . We had in mind building a networked synthesizer using RTP-MIDI in an embedded Raspberry Pi application. We really liked the features of modern Objective-C and thought it could provide a nice experience for audio programming. We purposely chose to focus on Linux and its ALSA interface. We wanted to avoid the layers of abstraction provided by the PortAudio project or JUCE. We also wanted to avoid dependence on a sound server like Pulse or JACK simply to reduce dependencies. We wanted to keep the toolkit lean and easy to set up. These decisions helped make the toolkit simple to use for simple things. Along the way we learned a lot and discovered some interesting capabilities. Hopefully this project will help share the things we learned. Concurrency One of the things that makes audio programming difficult is the management of concurrency. In an audio application like a synthesizer, there is usually a thread dedicated to rendering the audio, a thread dedicated to MIDI, a thread managing the GUI, and possibly other threads performing background tasks. Apple's GCD (Grand Central Dispatch) aka \"dispatch queues\" help simplify concurrency, and they are a first-class part of GNUstep's Objective-C environment. Instead of writing code that manages threads and locks, your code is organized into blocks that execute sequentially in a dispatch queue concurrency domain. The syntactic sugar of \"blocks\" in C lets you write code that executes in one concurrency domain and launches an operation in another. This replaces a cross-thread operation with something simpler. One of the design goals of the McLaren Synth Kit it to elevate audio programming to the realm of dispatch events, and to hide threads and locks as much as possible. We think we have succeeded. Memory Management The McLaren Synth Kit is designed to take full advantage of ARC (Automatic Reference Counting) for memory reclamation. A synthesizer with polyphony is usually rendering many different sounds at once. Each sound has a starting event (its \"attack\") and eventually its \"decay\" and \"release.\" It's convenient to think of sounds as being allocated and deallocated. While they are allocated, they are rendered to produce a sound. With the Synth Kit, a sound unit (called a \"Voice\") is dynamically allocated and configured, and then is handed to the audio thread for rendering. After the Voice is done playing, the audio thread marks it a reclaimable. A different background thread periodically checks for reclaimable Voices and gives them back to ARC. This puts the memory of the Voice back in the allocation pool. These details are handled by the Synth Kit. When you're writing an audio program using the kit, you only need to allocate and configure voices. Once you're done with a sound, Synth Kit takes care of cleaning up. Objective-C is C Objective-C is strictly a superset of C. This means that C structs and functions can be used directly in Objective-C programs. ALSA makes use of opaque C pointers as well as C structs for defining events and messages. These things are the \"language\" of ALSA. The Mclaren Synth Kit exposes many of these details where it adds no value to abstract them. Objective-C objects can be referenced as C structs. This capability can lead to dangerous programming territory, but when used carefully it is very powerful. In the Mclaren Synth Kit, we take advantage of the memory management and property capabilities of Objective-C objects, but we can (safely) pass references to these structs to low-level C functions. C functions can access Objective-C instance variables directly, because the memory layout is known by both C and Objective-C. Error Handling We tried to normalize errors that arise in a number of contexts. Wherever possible, errors exposed as NSError objects. Initializers that may encounter an error have an extra error:(NSError**) argument in which to describe the problem. A Brief Tour of the Components The tool kit is divided into two layers. The Alsa Sound Kit (ASK) provides minimal Objective-C wrappers around ALSA sound and MIDI sequencer devices. It also encapsulates standard algorithms on these things, so that instead of writing code to play a sound or enumerate the sequencers in your system, you call a method or set up a block. The major components of the Alsa Sound Kit are described below. ASKPcm - open an ALSA audio device and play or capture sounds ASKPcmList - list the ALSA audio devices in your system ASKSeq - open an ALSA MIDI Sequencer device and play or capture MIDI events ASKSeqList - list the ALSA MIDI devices in your system The Mclaren Synth Kit (MSK) further abstracts audio devices into an object called a \"Audio Context.\" An Audio Context can render audio pipelines that are described as a connected graph of audio elements, each of which is called a \"Voice\". A Voice can be a simple oscillator, an oscillator controlled by an envelope, or a more complex graph of multiple oscillators, envelopes and filters. A Voice can play for ever, it can stop after a specified time or it can be sent a noteOff message. Once created, a Voice is handed over to a Context for rendering. The Context manages the rendering and mixing of multiple Voices. In a typical use, a Voice is allocated for each MIDI note sounding. When a Voice is done playing, the Context removes the Voice and arranges for the reclamation of its memory. The major components of the Mclaren Synth Kit are described below. MSKContext - an audio context for rendering Voices or capturing sounds into a Voice MSKLinEnvelope, MSKExpEnvelope - linear and exponential envelope generators MSKSinFixedOscillator - a simple sinusoidal oscillator MSKGeneralOscillator - Sin, Squarewave, Triangle, Saw oscillator MSKPhaseDistortionOscillator - an oscillator who phase is controlled by another Voice The parameters of the various kinds of voices are continuously variable. To allow multiple voices to share configuration parameters (for example, all of the notes playing in a MIDI channel to have the same \"transpose\" or \"pitchbend\" setting) the continously variable parameters of voices are separated into objects called Models. The models in the system are listed below. MSKEnvelopeModel MSKOscillatorModel MSKReverbModel Aside: Models have a special role in the system: they provide read-only \"C\" variables that the Voices read from the audio thread. And for each \"C\" variable, they expose an Objective-C \"property\" for reading and writing that variable as an NSNumber Object. Essentially, they models are responsible for \"boxing\" and \"unboxing\" values. Models also know how to save and restore their state in NSUserDefaults . To round out the system, there is also a Metronome utility. The Metronome allocates an ALSA MIDI Sequencer and uses the MIDI event queue to schedule Metronome events. The Metronome produces callbacks on the beat and MIDI clock. The parameters of the Metronome are continuously variable, and they are separated out into a Model, just as the voices are. MSKMetronome MSKMetronomeModel A Complete Example Shown below is a complete example of a program that opens a device and plays a sound using a MSKContext . The top of the program opens a new Audio Context, configures it and starts it running. Dispatch queue calls are used to schedule events. At time t=1 second, a block is launched that creates a new sound and adds it to the context. This starts the sound playing until its release time has passed. At time t=2 seconds, a block is launched that closes the context. Lastly at time t=4 seconds a block is launched to exit the program. This program is available in the msk-examples directory and is named tiny.m . You can compile it yourself and try it out. int main(int argc, char *argv[]) { // Desired audio context parameters MSKContextRequest *request = [[MSKContextRequest alloc] init]; request.rate = 44000; request.persize = 1024; request.periods = 2; NSString *devName = @\"default\"; NSError *error; BOOL ok; // Create an audio context on the 'default' device for playback MSKContext *ctx = [[MSKContext alloc] initWithName:devName andStream:SND_PCM_STREAM_PLAYBACK error:&error]; if (error != nil) { NSLog(@\"MSKContext init error:%@\", error); exit(1); } // Configure the context with the request ok = [ctx configureForRequest:request error:&error]; if (ok == NO) { NSLog(@\"MSKContext configure error:%@\", error); exit(1); } // Start the context ok = [ctx startWithError:&error]; if (ok == NO) { NSLog(@\"MSKContext starting error:%@\", error); exit(1); } // Create and sound a Voice after 1 second dispatch_time_t attackt = dispatch_time(DISPATCH_TIME_NOW, 1.0*NSEC_PER_SEC); dispatch_after(attackt, dispatch_get_main_queue(), ^{ MSKOscillatorModel *oscmodel1 = [[MSKOscillatorModel alloc] initWithName:@\"osc1\"]; oscmodel1.osctype = @(MSK_OSCILLATOR_TYPE_TRIANGLE); MSKEnvelopeModel *envmodel1 = [[MSKEnvelopeModel alloc] initWithName:@\"env1\"]; envmodel1.sustain = @(1.0); envmodel1.rel = @(1.5); MSKExpEnvelope *env = [[MSKExpEnvelope alloc] initWithCtx:ctx]; env.oneshot = YES; env.shottime = 0.05; env.model = envmodel1; MSKGeneralOscillator *osc = [[MSKGeneralOscillator alloc] initWithCtx:ctx]; osc.iNote = @(60); osc.sEnvelope = env; osc.model = oscmodel1; // Hand the oscillator over to the context for rendering [ctx addVoice:osc]; }); // Close the context after two seconds dispatch_time_t after = dispatch_time(DISPATCH_TIME_NOW, 2.0*NSEC_PER_SEC); dispatch_after(after, dispatch_get_main_queue(), ^{ [ctx close]; }); // Exit after four seconds dispatch_time_t after4 = dispatch_time(DISPATCH_TIME_NOW, 4.0*NSEC_PER_SEC); dispatch_after(after4, dispatch_get_main_queue(), ^{ exit(0); }); // Run forever [[NSRunLoop currentRunLoop] run]; } Summary This chapter gives a brief introduction to the design goals of the Mclaren Synth Kit. The kit uses the concurrency and memory management features of modern Objective-C to provide a comfortable environment for audio programming on Linux. An example program showed how to play a synthesized sound consisting of an envelope generator and an oscillator playing a middle-C. Hopefully this was intriguing enough that you want to find out more. We didn't explain the details of how audio objects are defined and configured using the Mclaren Synth Kit, and we didn't explain the rules for how \"Voices\" (the audio objects of the MSK) can be combined to create more complex sounds. We also didn't cover how to find audio devices on your computer (i.e. - external USB sound cards) and how to use them. All of these will be covered in later chapters.","title":"Introduction to the Mclaren Synth Kit"},{"location":"XXintro/#introduction-to-the-mclaren-synth-kit","text":"The McLaren Synth Kit aims to bring audio programming to Objective-C on Linux. It consists of two parts. Alsa Sound Kit (ASK) - Obj-C wrappers around Alsa sound and MIDI devices Mclaren Synth Kit (MSK) - a more abstract sound \"context\" and synthesizer construction operators like oscillators, envelopes and effects. These tool kits are distributed as header files and static libraries compiled for specific compiler and runtime combinations. FYI: These are the same libraries McLaren Labs uses to construct the synthesizers that are available for free download at https://mclarenlabs.com .","title":"Introduction to the Mclaren Synth Kit"},{"location":"XXintro/#project-home","text":"The documentation you are reading here is from the /mkdocs directory of the McLaren Synth Kit repository on github. https://github.com/mclarenlabs/McLarenSynthKit","title":"Project Home"},{"location":"XXintro/#background","text":"This project is partially inspired by experiences with audio systems on IOS, OSX and Webkit and a NeXT project called MusicKit . We had in mind building a networked synthesizer using RTP-MIDI in an embedded Raspberry Pi application. We really liked the features of modern Objective-C and thought it could provide a nice experience for audio programming. We purposely chose to focus on Linux and its ALSA interface. We wanted to avoid the layers of abstraction provided by the PortAudio project or JUCE. We also wanted to avoid dependence on a sound server like Pulse or JACK simply to reduce dependencies. We wanted to keep the toolkit lean and easy to set up. These decisions helped make the toolkit simple to use for simple things. Along the way we learned a lot and discovered some interesting capabilities. Hopefully this project will help share the things we learned.","title":"Background"},{"location":"XXintro/#concurrency","text":"One of the things that makes audio programming difficult is the management of concurrency. In an audio application like a synthesizer, there is usually a thread dedicated to rendering the audio, a thread dedicated to MIDI, a thread managing the GUI, and possibly other threads performing background tasks. Apple's GCD (Grand Central Dispatch) aka \"dispatch queues\" help simplify concurrency, and they are a first-class part of GNUstep's Objective-C environment. Instead of writing code that manages threads and locks, your code is organized into blocks that execute sequentially in a dispatch queue concurrency domain. The syntactic sugar of \"blocks\" in C lets you write code that executes in one concurrency domain and launches an operation in another. This replaces a cross-thread operation with something simpler. One of the design goals of the McLaren Synth Kit it to elevate audio programming to the realm of dispatch events, and to hide threads and locks as much as possible. We think we have succeeded.","title":"Concurrency"},{"location":"XXintro/#memory-management","text":"The McLaren Synth Kit is designed to take full advantage of ARC (Automatic Reference Counting) for memory reclamation. A synthesizer with polyphony is usually rendering many different sounds at once. Each sound has a starting event (its \"attack\") and eventually its \"decay\" and \"release.\" It's convenient to think of sounds as being allocated and deallocated. While they are allocated, they are rendered to produce a sound. With the Synth Kit, a sound unit (called a \"Voice\") is dynamically allocated and configured, and then is handed to the audio thread for rendering. After the Voice is done playing, the audio thread marks it a reclaimable. A different background thread periodically checks for reclaimable Voices and gives them back to ARC. This puts the memory of the Voice back in the allocation pool. These details are handled by the Synth Kit. When you're writing an audio program using the kit, you only need to allocate and configure voices. Once you're done with a sound, Synth Kit takes care of cleaning up.","title":"Memory Management"},{"location":"XXintro/#objective-c-is-c","text":"Objective-C is strictly a superset of C. This means that C structs and functions can be used directly in Objective-C programs. ALSA makes use of opaque C pointers as well as C structs for defining events and messages. These things are the \"language\" of ALSA. The Mclaren Synth Kit exposes many of these details where it adds no value to abstract them. Objective-C objects can be referenced as C structs. This capability can lead to dangerous programming territory, but when used carefully it is very powerful. In the Mclaren Synth Kit, we take advantage of the memory management and property capabilities of Objective-C objects, but we can (safely) pass references to these structs to low-level C functions. C functions can access Objective-C instance variables directly, because the memory layout is known by both C and Objective-C.","title":"Objective-C is C"},{"location":"XXintro/#error-handling","text":"We tried to normalize errors that arise in a number of contexts. Wherever possible, errors exposed as NSError objects. Initializers that may encounter an error have an extra error:(NSError**) argument in which to describe the problem.","title":"Error Handling"},{"location":"XXintro/#a-brief-tour-of-the-components","text":"The tool kit is divided into two layers. The Alsa Sound Kit (ASK) provides minimal Objective-C wrappers around ALSA sound and MIDI sequencer devices. It also encapsulates standard algorithms on these things, so that instead of writing code to play a sound or enumerate the sequencers in your system, you call a method or set up a block. The major components of the Alsa Sound Kit are described below. ASKPcm - open an ALSA audio device and play or capture sounds ASKPcmList - list the ALSA audio devices in your system ASKSeq - open an ALSA MIDI Sequencer device and play or capture MIDI events ASKSeqList - list the ALSA MIDI devices in your system The Mclaren Synth Kit (MSK) further abstracts audio devices into an object called a \"Audio Context.\" An Audio Context can render audio pipelines that are described as a connected graph of audio elements, each of which is called a \"Voice\". A Voice can be a simple oscillator, an oscillator controlled by an envelope, or a more complex graph of multiple oscillators, envelopes and filters. A Voice can play for ever, it can stop after a specified time or it can be sent a noteOff message. Once created, a Voice is handed over to a Context for rendering. The Context manages the rendering and mixing of multiple Voices. In a typical use, a Voice is allocated for each MIDI note sounding. When a Voice is done playing, the Context removes the Voice and arranges for the reclamation of its memory. The major components of the Mclaren Synth Kit are described below. MSKContext - an audio context for rendering Voices or capturing sounds into a Voice MSKLinEnvelope, MSKExpEnvelope - linear and exponential envelope generators MSKSinFixedOscillator - a simple sinusoidal oscillator MSKGeneralOscillator - Sin, Squarewave, Triangle, Saw oscillator MSKPhaseDistortionOscillator - an oscillator who phase is controlled by another Voice The parameters of the various kinds of voices are continuously variable. To allow multiple voices to share configuration parameters (for example, all of the notes playing in a MIDI channel to have the same \"transpose\" or \"pitchbend\" setting) the continously variable parameters of voices are separated into objects called Models. The models in the system are listed below. MSKEnvelopeModel MSKOscillatorModel MSKReverbModel Aside: Models have a special role in the system: they provide read-only \"C\" variables that the Voices read from the audio thread. And for each \"C\" variable, they expose an Objective-C \"property\" for reading and writing that variable as an NSNumber Object. Essentially, they models are responsible for \"boxing\" and \"unboxing\" values. Models also know how to save and restore their state in NSUserDefaults . To round out the system, there is also a Metronome utility. The Metronome allocates an ALSA MIDI Sequencer and uses the MIDI event queue to schedule Metronome events. The Metronome produces callbacks on the beat and MIDI clock. The parameters of the Metronome are continuously variable, and they are separated out into a Model, just as the voices are. MSKMetronome MSKMetronomeModel","title":"A Brief Tour of the Components"},{"location":"XXintro/#a-complete-example","text":"Shown below is a complete example of a program that opens a device and plays a sound using a MSKContext . The top of the program opens a new Audio Context, configures it and starts it running. Dispatch queue calls are used to schedule events. At time t=1 second, a block is launched that creates a new sound and adds it to the context. This starts the sound playing until its release time has passed. At time t=2 seconds, a block is launched that closes the context. Lastly at time t=4 seconds a block is launched to exit the program. This program is available in the msk-examples directory and is named tiny.m . You can compile it yourself and try it out. int main(int argc, char *argv[]) { // Desired audio context parameters MSKContextRequest *request = [[MSKContextRequest alloc] init]; request.rate = 44000; request.persize = 1024; request.periods = 2; NSString *devName = @\"default\"; NSError *error; BOOL ok; // Create an audio context on the 'default' device for playback MSKContext *ctx = [[MSKContext alloc] initWithName:devName andStream:SND_PCM_STREAM_PLAYBACK error:&error]; if (error != nil) { NSLog(@\"MSKContext init error:%@\", error); exit(1); } // Configure the context with the request ok = [ctx configureForRequest:request error:&error]; if (ok == NO) { NSLog(@\"MSKContext configure error:%@\", error); exit(1); } // Start the context ok = [ctx startWithError:&error]; if (ok == NO) { NSLog(@\"MSKContext starting error:%@\", error); exit(1); } // Create and sound a Voice after 1 second dispatch_time_t attackt = dispatch_time(DISPATCH_TIME_NOW, 1.0*NSEC_PER_SEC); dispatch_after(attackt, dispatch_get_main_queue(), ^{ MSKOscillatorModel *oscmodel1 = [[MSKOscillatorModel alloc] initWithName:@\"osc1\"]; oscmodel1.osctype = @(MSK_OSCILLATOR_TYPE_TRIANGLE); MSKEnvelopeModel *envmodel1 = [[MSKEnvelopeModel alloc] initWithName:@\"env1\"]; envmodel1.sustain = @(1.0); envmodel1.rel = @(1.5); MSKExpEnvelope *env = [[MSKExpEnvelope alloc] initWithCtx:ctx]; env.oneshot = YES; env.shottime = 0.05; env.model = envmodel1; MSKGeneralOscillator *osc = [[MSKGeneralOscillator alloc] initWithCtx:ctx]; osc.iNote = @(60); osc.sEnvelope = env; osc.model = oscmodel1; // Hand the oscillator over to the context for rendering [ctx addVoice:osc]; }); // Close the context after two seconds dispatch_time_t after = dispatch_time(DISPATCH_TIME_NOW, 2.0*NSEC_PER_SEC); dispatch_after(after, dispatch_get_main_queue(), ^{ [ctx close]; }); // Exit after four seconds dispatch_time_t after4 = dispatch_time(DISPATCH_TIME_NOW, 4.0*NSEC_PER_SEC); dispatch_after(after4, dispatch_get_main_queue(), ^{ exit(0); }); // Run forever [[NSRunLoop currentRunLoop] run]; }","title":"A Complete Example"},{"location":"XXintro/#summary","text":"This chapter gives a brief introduction to the design goals of the Mclaren Synth Kit. The kit uses the concurrency and memory management features of modern Objective-C to provide a comfortable environment for audio programming on Linux. An example program showed how to play a synthesized sound consisting of an envelope generator and an oscillator playing a middle-C. Hopefully this was intriguing enough that you want to find out more. We didn't explain the details of how audio objects are defined and configured using the Mclaren Synth Kit, and we didn't explain the rules for how \"Voices\" (the audio objects of the MSK) can be combined to create more complex sounds. We also didn't cover how to find audio devices on your computer (i.e. - external USB sound cards) and how to use them. All of these will be covered in later chapters.","title":"Summary"},{"location":"ask-midi/","text":"Alsa Sound Kit The Alsa Sound Kit (ASK) provides Objective-C wrappers around ALSA MIDI devices and ALSA PCM (sound) devices. The Alsa Sound Kit depends on libasound.so and not much else except Obj-C and Foundation. The Alsa Sound Kit can be used without the Mclaren Synth Kit. A dependency graph is shown below. The purpose of the Alsa Sound Kit is to make it easier to use ALSA MIDI and Sound devices by doing more than wrapping the C-level ALSA functions. The Alsa Sound Kit provides subroutines for enumerating the devices in your system and for monitoring their status. The Alsa Sound Kit relieves you (the programmer) from managing threads. Instead, you set up your program to respond to MIDI events on a dispatch queue, or to respond to Audio events in a block. ALSA MIDI System Overview ALSA can expose MIDI devices as either \"Sequencer\" objects or \"Raw MIDI\" objects. Raw MIDI exposes the MIDI devices of sound cards and and sends and receives raw MIDI byte sequences. Sequencer objects are a more abstract representation of the MIDI system. At the Sequencer level, the MIDI system consists of a collection of \"clients\". A \"client\" has one or more \"ports\". Interconnections between \"ports\" are made to describe a MIDI processing graph. On Linux, the aconnect commands can help you about the connections between your MIDI Sequencer clients. To list the clients on your system try the command below. $ aconnect -i -o -l client 0: 'System' [type=kernel] 0 'Timer ' 1 'Announce ' client 14: 'Midi Through' [type=kernel] 0 'Midi Through Port-0' client 24: 'Launchkey 25' [type=kernel,card=2] 0 'Launchkey 25 MIDI 1' 1 'Launchkey 25 MIDI 2' client 130: 'Virtual Keyboard' [type=user,pid=77517] 0 'Virtual Keyboard' All systems have \"client 0\" and client 14\". Client 0 is a special System Client that announces events about the MIDI system itself. It announces the insertion and deletion of new devices in the system. Client 14 is another special client that is a MIDI pass through. On our system, we are running a GUI \"Virtual Keyboard\" program called vkeybd that is exposed as client 130. We have also plugged in a USB keyboard (a Novation LaunchKey 25) and that is exposed as client 24. The aseqdump command on Linux is a command line MIDI monitor. Use it to watch the events flowing through a specific port on a client. For instance, you can see the events produced by a MIDI USB keyboard as shown below. In this example, we played \"C-D-E-F-G\" on the keyboard. $ aseqdump -p 24:0 Waiting for data. Press Ctrl+C to end. Source Event Ch Data 24:0 Note on 0, note 60, velocity 33 24:0 Note off 0, note 60, velocity 49 24:0 Note on 0, note 62, velocity 38 24:0 Note on 0, note 64, velocity 37 24:0 Note off 0, note 62, velocity 38 24:0 Note on 0, note 65, velocity 29 24:0 Note off 0, note 64, velocity 43 24:0 Note off 0, note 65, velocity 68 24:0 Note on 0, note 67, velocity 48 24:0 Note off 0, note 67, velocity 76 ALSA MIDI Sequencer devices can also be referred to by name, or by partial name. Instead of referencing client \"24\" we can obtain the same effect by using the client name \"Launchkey 25\" or just a unique client-name prefix like \"Launch\". Note that because there are spaces in the name, you need to use the correct quoting. $ aseqdump -p 'Launchkey 25:0' Waiting for data. Press Ctrl+C to end. Source Event Ch Data 24:0 Note on 0, note 60, velocity 17 24:0 Note off 0, note 60, velocity 56 24:0 Note on 0, note 62, velocity 45 24:0 Note off 0, note 62, velocity 74 $ aseqdump -p Launch:0 Waiting for data. Press Ctrl+C to end. Source Event Ch Data 24:0 Note on 0, note 60, velocity 60 24:0 Note off 0, note 60, velocity 45 24:0 Note on 0, note 62, velocity 34 24:0 Note off 0, note 62, velocity 74 The MIDI \"System Client\" (client 0) is special. Instead of handling note events, this sequencer handles timing events and events about the system on its Port 1. Run the command below and unplug the USB MIDI Keyboard and then plug it back in. The System Client announces what is happening. In the example below you see the following thing: the \"Port Subscription\" is the aseqdump command itself registering with client-0:port-0. The rest of the output is information about Client 24 (our Launchkey 25). $ aseqdump -p 0:1 Waiting for data. Press Ctrl+C to end. Source Event Ch Data 0:1 Port subscribed 0:1 -> 128:0 0:1 Port exit 24:0 0:1 Port exit 24:1 0:1 Client exit client 24 0:1 Client start client 24 0:1 Port start 24:0 0:1 Port start 24:1 The aconnect command is usually used to make connections between clients and ports. We can explore its use with just a keyboard and the aseqdump command to illustrate how it works. First, start the aseqdump command running in one window with no arguments, like this. $ aseqdump Waiting for data at port 128:0. Press Ctrl+C to end. Source Event Ch Data We can see that a new client was created with number 128. This is a Sequencer Client created for the aseqdump command itself. Now, in another window, make a connection from the external keyboard to Client 128. $ aconnect 24:0 128:0 In the aseqdump window you should see some output if you play a few notes on the keyboard. 24:0 Note on 0, note 60, velocity 31 24:0 Note off 0, note 60, velocity 35 24:0 Note on 0, note 62, velocity 50 24:0 Note off 0, note 62, velocity 72 Now, \"disconnect\" the connection you just made. $ aconnect -d 24:0 128:0 In the 'aseqdump` window you will see the following message. 0:1 Port unsubscribed 24:0 -> 128:0 Note that because you can refer to clients with their names, you can also set up the connection shown above in this way. $ aconnect Launch:0 aseqdump:0 $ aconnect -d Launch:0 aseqdump:0 ALSA MIDI System Overview Summary This section gave an introduction to the concepts of the ALSA MIDI Sequencer level interface using command line tools and an external sequencer. Using these tools, we explored Clients and Ports. A Sequencer Client corresponds to a running program (i.e. aseqdump ) or a virtual or external keyboard. A Port is the source or destination of an event stream on a Client. The command-line tools aconnect and aseqdump are helpful tools for exploring the state of the Linux MIDI system and for observing events flowing through it. With the concepts of Clients, Ports and Connections firmly understood, the Alsa Sound Kit MIDI system is straightforward to explain. Note: the source code of the aconnect and aseqdump programs themselves are excellent for learning about the ALSA MIDI System. Look them up on the internet. ASK Seq An ASKSeq object is the Alsa Sound Kit wrapper for an ALSA Sequencer client. An ASKSeq produces MIDI events by calling a block passed to its addListener method. An ASKSeq may have many listeners. It's easy to use an ASKSeq to mimic the operation of the aseqdump command. The program below is from examples-ask/askseqdump.m in the project folder. The alloc/init line creates a new sequencer with default options. The addListener method call adds a callback block that logs the received events. int main(int argc, char *argv[]) { NSError *error; ASKSeq *seq = [[ASKSeq alloc] initWithError:&error]; [seq addListener:^(NSArray<ASKSeqEvent*> *events) { for (ASKSeqEvent* e in events) { NSLog(@\"%@\", e); } }]; [[NSRunLoop mainRunLoop] run]; } You can use this program to monitor MIDI events. Since there is no command line handling, you have to set up a connection externally with aconnect . $ aconnect Launch:0 askseqdump:0 Example output is shown below. (Note: if you are running VSCode there is already a build and test task configured to compile and run this program!) $ ./askseqdump 2020-12-28 10:08:25.900 askseqdump[85570:85587] 0:1 0 Port subscribed 24:0 -> 128:0 2020-12-28 10:08:26.884 askseqdump[85570:85587] 24:0 0 Note on 0, note 60, velocity 43 2020-12-28 10:08:27.033 askseqdump[85570:85587] 24:0 0 Note off 0, note 60, velocity 44 2020-12-28 10:08:27.104 askseqdump[85570:85587] 24:0 0 Note on 0, note 62, velocity 56 2020-12-28 10:08:27.262 askseqdump[85570:85587] 24:0 0 Note off 0, note 62, velocity 42 2020-12-28 10:08:27.287 askseqdump[85570:85587] 24:0 0 Note on 0, note 64, velocity 77 2020-12-28 10:08:27.382 askseqdump[85570:85587] 24:0 0 Note off 0, note 64, velocity 65 ^C There are a couple of interesting things about this program. A new sequencer client is created with default options that allow it to process events bidirectionally (more on that later) and with a default name (that we will change). The sequencer is also created with a default port and a default ALSA timing queue. Behind the scenes, the SEQ file descriptor is registered as a new dispatch source. A shared dispatch queue named \"midi\" has been created as a high-priority queue for processing MIDI events. The handler for the dispatch sources is a block that wraps each of the ALSA low-level C events in an Objective-C object called an ASKSEqEvent . The list of events produced at one time tick are gathered into an NSArray , and this is what is presented to our \"Listener\" block. Automatic Reference Counting (ARC) means our program does not need to manage deallocation or freeing of MIDI event objects. the Objective-C objects of the Alsa Sound Kit have useful description methods defined. When an ASKSeqEvent is printed using NSLog , this description is printed. As a result, writing a MIDI monitor is pretty easy using the Alsa Sound Kit. Configuring an ASK Seq - ASK Seq Options The default way that an ASKSeq is configured is through an ASKSeqOptions object. This object provides default values for the following things: sequencer name - a C string sequencer_type - ALSA \"default\" type sequencer_streams - DUPLEX sequencer_mode - NONBLOCK port_name - a C string port_caps - READ and WRITE port_type - GENERIC queue_name - a C string queue_temp - 60 BPM queue_resolution - 1000 ppq To change any of these from the default, allocate a new ASKSeqOptions object and override the parameter. Then open up the ASKSeq using the initWithOptions:error: method. In the following, we've modified the program above to be more sensible. ASKSeqOptions *options = [[ASKSeqOptions alloc] init]; options->_sequencer_name = \"askseqdump\"; NSError *error; ASKSeq *seq = [[ASKSeq alloc] initWithOptions:options error:&error]; The other thing that it oftentimes makes sense to override is the default \"Port\" name and its capabilities (READ or WRITE, etc). Making Connections The connectFrom:port:error: and connectTo:port:error: methods of an ASKSeq allow it to connect from or to another client at a specified port. We could modify our MIDI dumping program above to listen for events from the external keyboard at 24:0 as shown here. ASKSeq *seq = [[ASKSeq alloc] initWithOptions:options error:&error]; [seq connectFrom:24 port:0 error:&error]; // create a connection! [seq addListener:^(NSArray<ASKSeqEvent*> *events) { ... }]; There is a useful helper for parsing a MIDI sequencer address from a string. This is the parseAddress:error: method of the ASKSeq . This method taks an NSString as an argument. Using it, we could modify our MIDI dumping program to listen for events from the Launchkey keyboard like this. Note: the parseAddress:error: method understands client-name abbreviations because it uses snd_seq_parse_address under the hood. ASKSeq *seq = [[ASKSeq alloc] initWithOptions:options error:&error]; ASKSeqAddr *addr = [seq parseAddress:@\"Launch:0\" error:&error]; // unique prefix of our keyboard [seq connectFrom:addr.client port:addr.port error:&error]; // .client and .port properties [seq addListener:^(NSArray<ASKSeqEvent*> *events) { ... }]; ASK Seq Event The ALSA Seq subsystem communicates MIDI events by a rich structure defined by the type snd_seq_event_t . In working with ALSA Sequencer events it is eventually necessary to become familar with this structure definition in /usr/include/alsa/seq_event.h . Note: previous versions of the Alsa Sound Kit attempted to abstract the use of the snd_seq_event_t structure through Objective-C properties and getters and setters. The result was complicated and did not add much value. We decided to eliminate property abstraction for the time being. When Alsa Sound Kit receives a snd_seq_event_t from ALSA, it performs a value copy of the struct into an Objective-C object called ASKSEqEvent . Its declaration is shown here. In this way, the MIDI event data is managed by the Objective-C memory manager. @interface ASKSeqEvent : NSObject { NSData *_data; // backing store for ext (sysex) data @public snd_seq_event_t _ev; } @property (nonatomic, readwrite, getter=getExt, setter=setExt:) NSData *ext; // get and set ext data @end For SysEx data, the Alsa Sound Kit helps mapping the bytes of an NSData object to and from the ev->data.ext.ptr . Handling Received Events The example program minisynth1.m decodes MIDI events to construct sounds. In general, a loop handling received MIDI events will look something like the following. Each different type of event uses the union fields of the struct in a different way. [seq addListener:^(NSArray<ASKSeqEvent*> *events) { for (ASKSeqEvent *e in events) { if (e->_ev.type == SND_SEQ_EVENT_NOTEON) { uint8_t chan = e->_ev.data.note.channel; uint8_t note = e->_ev.data.note.note; uint8_t vel = e->_ev.data.note.velocity; // ... do something } if (e->_ev.type == SND_SEQ_EVENT_NOTEOFF) { uint8_t chan = e->_ev.data.note.channel; uint8_t note = e->_ev.data.note.note; uint8_t vel = e->_ev.data.note.velocity; // ... do something } } }]; Creating and Sending an Event The opposite of receiving an event is creating an event. To create a new note event, allocate a blank ASKSEqEvent and then fill it out. The following snippet creates a NOTEON event that will be sent to all subscribers of our client. ASKSeqEvent *e = [[ASKSeqEvent alloc] init]; e->_ev.type = SND_SEQ_EVENT_NOTEON; snd_seq_ev_set_subs(&(e->_ev)); // all subscribers e->_ev.dest.port = 0; e->_ev.data.note.channel = chan; e->_ev.data.note.note = note; e->_ev.data.note.velocity = vel; e->_ev.queue = SND_SEQ_QUEUE_DIRECT; // no scheduling To send the event, use the outputDirect: method of an ASKSEq . [seq outputDirect:e]; To create a SysEx message, put the SysEx bytes in an NSData* and attach it to the ext field of the ASKSeqEvent before filling out the rest of the event and sending. ASKSeqEvent *e = [[ASKSeqEvent alloc] init]; e.ext = sysex; e->_ev.type = SND_SEQ_EVENT_SYSEX; snd_seq_ev_set_subs(&(e->_ev)); // all subscribers e->_ev.dest.port = 0; e->_ev.queue = SND_SEQ_QUEUE_DIRECT; // no scheduling [seq outputDirect:e]; Listing Available Seq Clients and Ports A special utility class called ASKSeqList is provided to list the available clients in the system. This class is pretty smart: when instantiated, it creates the current list of clients in the system, and then it registers for System ANNOUNCE events to be notified of clients and ports disconnecting and connecting. The class also allows for user notifications. Each Client in the system is described by an ASKSeqClientInfo (a simple wrapper around an ALSA snd_seq_client_info_t ). Each Port in the system is described by an ASKSeqPortInfo (a simple wrapper around an ALSA snd_seq_client_info_t ). Given an existing ASKSeq , a new ASKSeqList can be created as shown below. The clientinfos and portinfos are available directy as properties of the ASKSeqList . ASKSeqList *list = [[ASKSeqList alloc] initWithSeq:seq]; for (ASKSeqClientInfo *c in list.clientinfos) { NSLog(@\"%@\", c); } $ ./askseqlist 2020-12-28 13:36:28.381 askseqlist[89989:89989] CLIENT:0 Name:System Type:2 2020-12-28 13:36:28.381 askseqlist[89989:89989] CLIENT:14 Name:Midi Through Type:2 2020-12-28 13:36:28.381 askseqlist[89989:89989] CLIENT:24 Name:Launchkey 25 Type:2 2020-12-28 13:36:28.381 askseqlist[89989:89989] CLIENT:128 Name:askseqdump Type:1 2020-12-28 13:36:28.381 askseqlist[89989:89989] CLIENT:130 Name:Virtual Keyboard Type:1 It is also possible to list all of the ports in the system. for (ASKSeqPortInfo *p in list.portinfos) { NSLog(@\"%@\", p); } The snippet above produces the following on our system. $ ./askseqlist 2020-12-28 13:38:11.839 askseqlist[90222:90222] PORT Client:0 Port:0 Name:Timer cap:0 type:0 R:0 W:0 2020-12-28 13:38:11.839 askseqlist[90222:90222] PORT Client:0 Port:1 Name:Announce cap:0 type:0 R:0 W:0 2020-12-28 13:38:11.839 askseqlist[90222:90222] PORT Client:14 Port:0 Name:Midi Through Port-0 cap:655362 type:655362 R:0 W:0 2020-12-28 13:38:11.839 askseqlist[90222:90222] PORT Client:24 Port:0 Name:Launchkey 25 MIDI 1 cap:589826 type:589826 R:0 W:0 2020-12-28 13:38:11.839 askseqlist[90222:90222] PORT Client:24 Port:1 Name:Launchkey 25 MIDI 2 cap:589826 type:589826 R:0 W:0 2020-12-28 13:38:11.839 askseqlist[90222:90222] PORT Client:128 Port:0 Name:__port__ cap:2 type:2 R:0 W:0 2020-12-28 13:38:11.839 askseqlist[90222:90222] PORT Client:130 Port:0 Name:Virtual Keyboard cap:1048578 type:1048578 R:0 W:0 The ASKSeqList also keeps track of changes to the system and modifies clientinfos and portinfos as appropriate. Your code can register for callbacks to be notified of changes. The code below shows how. [list onClientAdded:^(ASKSeqClientInfo* c) { NSLog(@\"Client Added - %@\", c); }]; [list onClientDeleted:^(ASKSeqClientInfo* c) { NSLog(@\"Client Deleted - %@\", c); }]; [list onPortAdded:^(ASKSeqPortInfo* p) { NSLog(@\"Port Added - %@\", p); }]; [list onPortDeleted:^(ASKSeqPortInfo* p) { NSLog(@\"Port Deleted - %@\", p); }]; If we compile and run this on our system, and then unplug the Launchkey keyboard and plug it back in, we see the following output. 2020-12-28 13:46:35.994 askseqlist[90803:90805] Port Deleted - PORT Client:24 Port:0 Name:Launchkey 25 MIDI 1 cap:589826 type:589826 R:0 W:0 2020-12-28 13:46:35.994 askseqlist[90803:90805] Port Deleted - PORT Client:24 Port:1 Name:Launchkey 25 MIDI 2 cap:589826 type:589826 R:0 W:0 2020-12-28 13:46:35.994 askseqlist[90803:90805] Client Deleted - CLIENT:24 Name:Launchkey 25 Type:2 2020-12-28 13:46:38.973 askseqlist[90803:90805] Client Added - CLIENT:24 Name:Launchkey 25 Type:2 2020-12-28 13:46:38.973 askseqlist[90803:90805] Port Added - PORT Client:24 Port:0 Name:Launchkey 25 MIDI 1 cap:589826 type:589826 R:0 W:0 2020-12-28 13:46:38.973 askseqlist[90803:90805] Port Added - PORT Client:24 Port:1 Name:Launchkey 25 MIDI 2 cap:589826 type:589826 R:0 W:0 McLaren Labs sound software (for instance, the mclaren80 synthesizer) uses this capability with GUI code to keep the list of devices presented in a GUI dropdown list synchronized with the current state of the system even as new USB keyboards are plugged in. Summary This chapter described the features of the MIDI subsystem of the Alsa Sound Kit (\"ASK\"). We described how Clients and Ports represent the devices and connections in a Linux system and how you can view and manipulate these with the aconnect and aseqdump commands. We then showed how Clients and Ports and Connections can be managed in Objective-C code. An ASKSeq is an object representing a new Sequencer Client. It is created with a default Port and a pre-made queue for handling events. An ASKSeqOptions object can be used to customize a basic sequencer client with a specific client name or port name. We showed how to make and break connections with the connectFrom: , connectTo: , disconnectFrom: and disconnectTo: methods. MIDI events were introduced and we showed how to examine incoming events and how to construct new events for sending. Lastly, we showed how to write code that enumerates the Clients and Ports in the system and how to be notified about changing system topology as MIDI devices are added and deleted.","title":"Alsa Sound Kit - MIDI"},{"location":"ask-midi/#alsa-sound-kit","text":"The Alsa Sound Kit (ASK) provides Objective-C wrappers around ALSA MIDI devices and ALSA PCM (sound) devices. The Alsa Sound Kit depends on libasound.so and not much else except Obj-C and Foundation. The Alsa Sound Kit can be used without the Mclaren Synth Kit. A dependency graph is shown below. The purpose of the Alsa Sound Kit is to make it easier to use ALSA MIDI and Sound devices by doing more than wrapping the C-level ALSA functions. The Alsa Sound Kit provides subroutines for enumerating the devices in your system and for monitoring their status. The Alsa Sound Kit relieves you (the programmer) from managing threads. Instead, you set up your program to respond to MIDI events on a dispatch queue, or to respond to Audio events in a block.","title":"Alsa Sound Kit"},{"location":"ask-midi/#alsa-midi-system-overview","text":"ALSA can expose MIDI devices as either \"Sequencer\" objects or \"Raw MIDI\" objects. Raw MIDI exposes the MIDI devices of sound cards and and sends and receives raw MIDI byte sequences. Sequencer objects are a more abstract representation of the MIDI system. At the Sequencer level, the MIDI system consists of a collection of \"clients\". A \"client\" has one or more \"ports\". Interconnections between \"ports\" are made to describe a MIDI processing graph. On Linux, the aconnect commands can help you about the connections between your MIDI Sequencer clients. To list the clients on your system try the command below. $ aconnect -i -o -l client 0: 'System' [type=kernel] 0 'Timer ' 1 'Announce ' client 14: 'Midi Through' [type=kernel] 0 'Midi Through Port-0' client 24: 'Launchkey 25' [type=kernel,card=2] 0 'Launchkey 25 MIDI 1' 1 'Launchkey 25 MIDI 2' client 130: 'Virtual Keyboard' [type=user,pid=77517] 0 'Virtual Keyboard' All systems have \"client 0\" and client 14\". Client 0 is a special System Client that announces events about the MIDI system itself. It announces the insertion and deletion of new devices in the system. Client 14 is another special client that is a MIDI pass through. On our system, we are running a GUI \"Virtual Keyboard\" program called vkeybd that is exposed as client 130. We have also plugged in a USB keyboard (a Novation LaunchKey 25) and that is exposed as client 24. The aseqdump command on Linux is a command line MIDI monitor. Use it to watch the events flowing through a specific port on a client. For instance, you can see the events produced by a MIDI USB keyboard as shown below. In this example, we played \"C-D-E-F-G\" on the keyboard. $ aseqdump -p 24:0 Waiting for data. Press Ctrl+C to end. Source Event Ch Data 24:0 Note on 0, note 60, velocity 33 24:0 Note off 0, note 60, velocity 49 24:0 Note on 0, note 62, velocity 38 24:0 Note on 0, note 64, velocity 37 24:0 Note off 0, note 62, velocity 38 24:0 Note on 0, note 65, velocity 29 24:0 Note off 0, note 64, velocity 43 24:0 Note off 0, note 65, velocity 68 24:0 Note on 0, note 67, velocity 48 24:0 Note off 0, note 67, velocity 76 ALSA MIDI Sequencer devices can also be referred to by name, or by partial name. Instead of referencing client \"24\" we can obtain the same effect by using the client name \"Launchkey 25\" or just a unique client-name prefix like \"Launch\". Note that because there are spaces in the name, you need to use the correct quoting. $ aseqdump -p 'Launchkey 25:0' Waiting for data. Press Ctrl+C to end. Source Event Ch Data 24:0 Note on 0, note 60, velocity 17 24:0 Note off 0, note 60, velocity 56 24:0 Note on 0, note 62, velocity 45 24:0 Note off 0, note 62, velocity 74 $ aseqdump -p Launch:0 Waiting for data. Press Ctrl+C to end. Source Event Ch Data 24:0 Note on 0, note 60, velocity 60 24:0 Note off 0, note 60, velocity 45 24:0 Note on 0, note 62, velocity 34 24:0 Note off 0, note 62, velocity 74 The MIDI \"System Client\" (client 0) is special. Instead of handling note events, this sequencer handles timing events and events about the system on its Port 1. Run the command below and unplug the USB MIDI Keyboard and then plug it back in. The System Client announces what is happening. In the example below you see the following thing: the \"Port Subscription\" is the aseqdump command itself registering with client-0:port-0. The rest of the output is information about Client 24 (our Launchkey 25). $ aseqdump -p 0:1 Waiting for data. Press Ctrl+C to end. Source Event Ch Data 0:1 Port subscribed 0:1 -> 128:0 0:1 Port exit 24:0 0:1 Port exit 24:1 0:1 Client exit client 24 0:1 Client start client 24 0:1 Port start 24:0 0:1 Port start 24:1 The aconnect command is usually used to make connections between clients and ports. We can explore its use with just a keyboard and the aseqdump command to illustrate how it works. First, start the aseqdump command running in one window with no arguments, like this. $ aseqdump Waiting for data at port 128:0. Press Ctrl+C to end. Source Event Ch Data We can see that a new client was created with number 128. This is a Sequencer Client created for the aseqdump command itself. Now, in another window, make a connection from the external keyboard to Client 128. $ aconnect 24:0 128:0 In the aseqdump window you should see some output if you play a few notes on the keyboard. 24:0 Note on 0, note 60, velocity 31 24:0 Note off 0, note 60, velocity 35 24:0 Note on 0, note 62, velocity 50 24:0 Note off 0, note 62, velocity 72 Now, \"disconnect\" the connection you just made. $ aconnect -d 24:0 128:0 In the 'aseqdump` window you will see the following message. 0:1 Port unsubscribed 24:0 -> 128:0 Note that because you can refer to clients with their names, you can also set up the connection shown above in this way. $ aconnect Launch:0 aseqdump:0 $ aconnect -d Launch:0 aseqdump:0","title":"ALSA MIDI System Overview"},{"location":"ask-midi/#alsa-midi-system-overview-summary","text":"This section gave an introduction to the concepts of the ALSA MIDI Sequencer level interface using command line tools and an external sequencer. Using these tools, we explored Clients and Ports. A Sequencer Client corresponds to a running program (i.e. aseqdump ) or a virtual or external keyboard. A Port is the source or destination of an event stream on a Client. The command-line tools aconnect and aseqdump are helpful tools for exploring the state of the Linux MIDI system and for observing events flowing through it. With the concepts of Clients, Ports and Connections firmly understood, the Alsa Sound Kit MIDI system is straightforward to explain. Note: the source code of the aconnect and aseqdump programs themselves are excellent for learning about the ALSA MIDI System. Look them up on the internet.","title":"ALSA MIDI System Overview Summary"},{"location":"ask-midi/#ask-seq","text":"An ASKSeq object is the Alsa Sound Kit wrapper for an ALSA Sequencer client. An ASKSeq produces MIDI events by calling a block passed to its addListener method. An ASKSeq may have many listeners. It's easy to use an ASKSeq to mimic the operation of the aseqdump command. The program below is from examples-ask/askseqdump.m in the project folder. The alloc/init line creates a new sequencer with default options. The addListener method call adds a callback block that logs the received events. int main(int argc, char *argv[]) { NSError *error; ASKSeq *seq = [[ASKSeq alloc] initWithError:&error]; [seq addListener:^(NSArray<ASKSeqEvent*> *events) { for (ASKSeqEvent* e in events) { NSLog(@\"%@\", e); } }]; [[NSRunLoop mainRunLoop] run]; } You can use this program to monitor MIDI events. Since there is no command line handling, you have to set up a connection externally with aconnect . $ aconnect Launch:0 askseqdump:0 Example output is shown below. (Note: if you are running VSCode there is already a build and test task configured to compile and run this program!) $ ./askseqdump 2020-12-28 10:08:25.900 askseqdump[85570:85587] 0:1 0 Port subscribed 24:0 -> 128:0 2020-12-28 10:08:26.884 askseqdump[85570:85587] 24:0 0 Note on 0, note 60, velocity 43 2020-12-28 10:08:27.033 askseqdump[85570:85587] 24:0 0 Note off 0, note 60, velocity 44 2020-12-28 10:08:27.104 askseqdump[85570:85587] 24:0 0 Note on 0, note 62, velocity 56 2020-12-28 10:08:27.262 askseqdump[85570:85587] 24:0 0 Note off 0, note 62, velocity 42 2020-12-28 10:08:27.287 askseqdump[85570:85587] 24:0 0 Note on 0, note 64, velocity 77 2020-12-28 10:08:27.382 askseqdump[85570:85587] 24:0 0 Note off 0, note 64, velocity 65 ^C There are a couple of interesting things about this program. A new sequencer client is created with default options that allow it to process events bidirectionally (more on that later) and with a default name (that we will change). The sequencer is also created with a default port and a default ALSA timing queue. Behind the scenes, the SEQ file descriptor is registered as a new dispatch source. A shared dispatch queue named \"midi\" has been created as a high-priority queue for processing MIDI events. The handler for the dispatch sources is a block that wraps each of the ALSA low-level C events in an Objective-C object called an ASKSEqEvent . The list of events produced at one time tick are gathered into an NSArray , and this is what is presented to our \"Listener\" block. Automatic Reference Counting (ARC) means our program does not need to manage deallocation or freeing of MIDI event objects. the Objective-C objects of the Alsa Sound Kit have useful description methods defined. When an ASKSeqEvent is printed using NSLog , this description is printed. As a result, writing a MIDI monitor is pretty easy using the Alsa Sound Kit.","title":"ASK Seq"},{"location":"ask-midi/#configuring-an-ask-seq-ask-seq-options","text":"The default way that an ASKSeq is configured is through an ASKSeqOptions object. This object provides default values for the following things: sequencer name - a C string sequencer_type - ALSA \"default\" type sequencer_streams - DUPLEX sequencer_mode - NONBLOCK port_name - a C string port_caps - READ and WRITE port_type - GENERIC queue_name - a C string queue_temp - 60 BPM queue_resolution - 1000 ppq To change any of these from the default, allocate a new ASKSeqOptions object and override the parameter. Then open up the ASKSeq using the initWithOptions:error: method. In the following, we've modified the program above to be more sensible. ASKSeqOptions *options = [[ASKSeqOptions alloc] init]; options->_sequencer_name = \"askseqdump\"; NSError *error; ASKSeq *seq = [[ASKSeq alloc] initWithOptions:options error:&error]; The other thing that it oftentimes makes sense to override is the default \"Port\" name and its capabilities (READ or WRITE, etc).","title":"Configuring an ASK Seq - ASK Seq Options"},{"location":"ask-midi/#making-connections","text":"The connectFrom:port:error: and connectTo:port:error: methods of an ASKSeq allow it to connect from or to another client at a specified port. We could modify our MIDI dumping program above to listen for events from the external keyboard at 24:0 as shown here. ASKSeq *seq = [[ASKSeq alloc] initWithOptions:options error:&error]; [seq connectFrom:24 port:0 error:&error]; // create a connection! [seq addListener:^(NSArray<ASKSeqEvent*> *events) { ... }]; There is a useful helper for parsing a MIDI sequencer address from a string. This is the parseAddress:error: method of the ASKSeq . This method taks an NSString as an argument. Using it, we could modify our MIDI dumping program to listen for events from the Launchkey keyboard like this. Note: the parseAddress:error: method understands client-name abbreviations because it uses snd_seq_parse_address under the hood. ASKSeq *seq = [[ASKSeq alloc] initWithOptions:options error:&error]; ASKSeqAddr *addr = [seq parseAddress:@\"Launch:0\" error:&error]; // unique prefix of our keyboard [seq connectFrom:addr.client port:addr.port error:&error]; // .client and .port properties [seq addListener:^(NSArray<ASKSeqEvent*> *events) { ... }];","title":"Making Connections"},{"location":"ask-midi/#ask-seq-event","text":"The ALSA Seq subsystem communicates MIDI events by a rich structure defined by the type snd_seq_event_t . In working with ALSA Sequencer events it is eventually necessary to become familar with this structure definition in /usr/include/alsa/seq_event.h . Note: previous versions of the Alsa Sound Kit attempted to abstract the use of the snd_seq_event_t structure through Objective-C properties and getters and setters. The result was complicated and did not add much value. We decided to eliminate property abstraction for the time being. When Alsa Sound Kit receives a snd_seq_event_t from ALSA, it performs a value copy of the struct into an Objective-C object called ASKSEqEvent . Its declaration is shown here. In this way, the MIDI event data is managed by the Objective-C memory manager. @interface ASKSeqEvent : NSObject { NSData *_data; // backing store for ext (sysex) data @public snd_seq_event_t _ev; } @property (nonatomic, readwrite, getter=getExt, setter=setExt:) NSData *ext; // get and set ext data @end For SysEx data, the Alsa Sound Kit helps mapping the bytes of an NSData object to and from the ev->data.ext.ptr .","title":"ASK Seq Event"},{"location":"ask-midi/#handling-received-events","text":"The example program minisynth1.m decodes MIDI events to construct sounds. In general, a loop handling received MIDI events will look something like the following. Each different type of event uses the union fields of the struct in a different way. [seq addListener:^(NSArray<ASKSeqEvent*> *events) { for (ASKSeqEvent *e in events) { if (e->_ev.type == SND_SEQ_EVENT_NOTEON) { uint8_t chan = e->_ev.data.note.channel; uint8_t note = e->_ev.data.note.note; uint8_t vel = e->_ev.data.note.velocity; // ... do something } if (e->_ev.type == SND_SEQ_EVENT_NOTEOFF) { uint8_t chan = e->_ev.data.note.channel; uint8_t note = e->_ev.data.note.note; uint8_t vel = e->_ev.data.note.velocity; // ... do something } } }];","title":"Handling Received Events"},{"location":"ask-midi/#creating-and-sending-an-event","text":"The opposite of receiving an event is creating an event. To create a new note event, allocate a blank ASKSEqEvent and then fill it out. The following snippet creates a NOTEON event that will be sent to all subscribers of our client. ASKSeqEvent *e = [[ASKSeqEvent alloc] init]; e->_ev.type = SND_SEQ_EVENT_NOTEON; snd_seq_ev_set_subs(&(e->_ev)); // all subscribers e->_ev.dest.port = 0; e->_ev.data.note.channel = chan; e->_ev.data.note.note = note; e->_ev.data.note.velocity = vel; e->_ev.queue = SND_SEQ_QUEUE_DIRECT; // no scheduling To send the event, use the outputDirect: method of an ASKSEq . [seq outputDirect:e]; To create a SysEx message, put the SysEx bytes in an NSData* and attach it to the ext field of the ASKSeqEvent before filling out the rest of the event and sending. ASKSeqEvent *e = [[ASKSeqEvent alloc] init]; e.ext = sysex; e->_ev.type = SND_SEQ_EVENT_SYSEX; snd_seq_ev_set_subs(&(e->_ev)); // all subscribers e->_ev.dest.port = 0; e->_ev.queue = SND_SEQ_QUEUE_DIRECT; // no scheduling [seq outputDirect:e];","title":"Creating and Sending an Event"},{"location":"ask-midi/#listing-available-seq-clients-and-ports","text":"A special utility class called ASKSeqList is provided to list the available clients in the system. This class is pretty smart: when instantiated, it creates the current list of clients in the system, and then it registers for System ANNOUNCE events to be notified of clients and ports disconnecting and connecting. The class also allows for user notifications. Each Client in the system is described by an ASKSeqClientInfo (a simple wrapper around an ALSA snd_seq_client_info_t ). Each Port in the system is described by an ASKSeqPortInfo (a simple wrapper around an ALSA snd_seq_client_info_t ). Given an existing ASKSeq , a new ASKSeqList can be created as shown below. The clientinfos and portinfos are available directy as properties of the ASKSeqList . ASKSeqList *list = [[ASKSeqList alloc] initWithSeq:seq]; for (ASKSeqClientInfo *c in list.clientinfos) { NSLog(@\"%@\", c); } $ ./askseqlist 2020-12-28 13:36:28.381 askseqlist[89989:89989] CLIENT:0 Name:System Type:2 2020-12-28 13:36:28.381 askseqlist[89989:89989] CLIENT:14 Name:Midi Through Type:2 2020-12-28 13:36:28.381 askseqlist[89989:89989] CLIENT:24 Name:Launchkey 25 Type:2 2020-12-28 13:36:28.381 askseqlist[89989:89989] CLIENT:128 Name:askseqdump Type:1 2020-12-28 13:36:28.381 askseqlist[89989:89989] CLIENT:130 Name:Virtual Keyboard Type:1 It is also possible to list all of the ports in the system. for (ASKSeqPortInfo *p in list.portinfos) { NSLog(@\"%@\", p); } The snippet above produces the following on our system. $ ./askseqlist 2020-12-28 13:38:11.839 askseqlist[90222:90222] PORT Client:0 Port:0 Name:Timer cap:0 type:0 R:0 W:0 2020-12-28 13:38:11.839 askseqlist[90222:90222] PORT Client:0 Port:1 Name:Announce cap:0 type:0 R:0 W:0 2020-12-28 13:38:11.839 askseqlist[90222:90222] PORT Client:14 Port:0 Name:Midi Through Port-0 cap:655362 type:655362 R:0 W:0 2020-12-28 13:38:11.839 askseqlist[90222:90222] PORT Client:24 Port:0 Name:Launchkey 25 MIDI 1 cap:589826 type:589826 R:0 W:0 2020-12-28 13:38:11.839 askseqlist[90222:90222] PORT Client:24 Port:1 Name:Launchkey 25 MIDI 2 cap:589826 type:589826 R:0 W:0 2020-12-28 13:38:11.839 askseqlist[90222:90222] PORT Client:128 Port:0 Name:__port__ cap:2 type:2 R:0 W:0 2020-12-28 13:38:11.839 askseqlist[90222:90222] PORT Client:130 Port:0 Name:Virtual Keyboard cap:1048578 type:1048578 R:0 W:0 The ASKSeqList also keeps track of changes to the system and modifies clientinfos and portinfos as appropriate. Your code can register for callbacks to be notified of changes. The code below shows how. [list onClientAdded:^(ASKSeqClientInfo* c) { NSLog(@\"Client Added - %@\", c); }]; [list onClientDeleted:^(ASKSeqClientInfo* c) { NSLog(@\"Client Deleted - %@\", c); }]; [list onPortAdded:^(ASKSeqPortInfo* p) { NSLog(@\"Port Added - %@\", p); }]; [list onPortDeleted:^(ASKSeqPortInfo* p) { NSLog(@\"Port Deleted - %@\", p); }]; If we compile and run this on our system, and then unplug the Launchkey keyboard and plug it back in, we see the following output. 2020-12-28 13:46:35.994 askseqlist[90803:90805] Port Deleted - PORT Client:24 Port:0 Name:Launchkey 25 MIDI 1 cap:589826 type:589826 R:0 W:0 2020-12-28 13:46:35.994 askseqlist[90803:90805] Port Deleted - PORT Client:24 Port:1 Name:Launchkey 25 MIDI 2 cap:589826 type:589826 R:0 W:0 2020-12-28 13:46:35.994 askseqlist[90803:90805] Client Deleted - CLIENT:24 Name:Launchkey 25 Type:2 2020-12-28 13:46:38.973 askseqlist[90803:90805] Client Added - CLIENT:24 Name:Launchkey 25 Type:2 2020-12-28 13:46:38.973 askseqlist[90803:90805] Port Added - PORT Client:24 Port:0 Name:Launchkey 25 MIDI 1 cap:589826 type:589826 R:0 W:0 2020-12-28 13:46:38.973 askseqlist[90803:90805] Port Added - PORT Client:24 Port:1 Name:Launchkey 25 MIDI 2 cap:589826 type:589826 R:0 W:0 McLaren Labs sound software (for instance, the mclaren80 synthesizer) uses this capability with GUI code to keep the list of devices presented in a GUI dropdown list synchronized with the current state of the system even as new USB keyboards are plugged in.","title":"Listing Available Seq Clients and Ports"},{"location":"ask-midi/#summary","text":"This chapter described the features of the MIDI subsystem of the Alsa Sound Kit (\"ASK\"). We described how Clients and Ports represent the devices and connections in a Linux system and how you can view and manipulate these with the aconnect and aseqdump commands. We then showed how Clients and Ports and Connections can be managed in Objective-C code. An ASKSeq is an object representing a new Sequencer Client. It is created with a default Port and a pre-made queue for handling events. An ASKSeqOptions object can be used to customize a basic sequencer client with a specific client name or port name. We showed how to make and break connections with the connectFrom: , connectTo: , disconnectFrom: and disconnectTo: methods. MIDI events were introduced and we showed how to examine incoming events and how to construct new events for sending. Lastly, we showed how to write code that enumerates the Clients and Ports in the system and how to be notified about changing system topology as MIDI devices are added and deleted.","title":"Summary"},{"location":"ask-pcm/","text":"Alsa Sound Kit - PCM The Alsa Sound Kit (ASK) provides Objective-C wrappers around ALSA MIDI devices and ALSA PCM (sound) devices. This chapter describes how sound devices are configured and how sounds are played or captured. ALSA PCM System Overview The Advance Linux Sound Architecture (ALSA) library calls a device that can translate continuous sound waves into a series of samples a \"PCM\" (PCM stands for Pulse Code Modulation). ALSA PCM devices can PLAY and CAPTURE sounds. In your system, a sound capability is provided by a \"card.\" These days, a sound card is usually integrated directly into your laptop motherboard. When you plug a USB audio device into your system, that will appear as a card too. A card can have one or more devices. Some of the devices can play or capture audio, and there can be devices for MIDI or other functions too. For our purposes here, we'll be focusing just on the devices that play or capture audio. On a Linux computer, you can list sound devices using the aplay command. This command can also play audio samples. Try it out like this. $ aplay /usr/share/sounds/alsa/Front_Center.wav Playing WAVE '/usr/share/sounds/alsa/Front_Center.wav' : Signed 16 bit Little Endian, Rate 48000 Hz, Mono If all goes well, you'll hear a sound saying \"Front. Center.\" and you'll see the output above. To list the audio cards and devices in your system use aplay -l . On our laptop at McLaren Labs, it produces the following output. $ aplay -l **** List of PLAYBACK Hardware Devices **** card 0: HDMI [HDA Intel HDMI], device 3: HDMI 0 [HDMI 0] Subdevices: 1/1 Subdevice #0: subdevice #0 card 0: HDMI [HDA Intel HDMI], device 7: HDMI 1 [HDMI 1] Subdevices: 1/1 Subdevice #0: subdevice #0 card 0: HDMI [HDA Intel HDMI], device 8: HDMI 2 [HDMI 2] Subdevices: 1/1 Subdevice #0: subdevice #0 card 0: HDMI [HDA Intel HDMI], device 9: HDMI 3 [HDMI 3] Subdevices: 1/1 Subdevice #0: subdevice #0 card 0: HDMI [HDA Intel HDMI], device 10: HDMI 4 [HDMI 4] Subdevices: 1/1 Subdevice #0: subdevice #0 card 1: PCH [HDA Intel PCH], device 0: CS4208 Analog [CS4208 Analog] Subdevices: 1/1 Subdevice #0: subdevice #0 card 1: PCH [HDA Intel PCH], device 1: CS4208 Digital [CS4208 Digital] Subdevices: 1/1 Subdevice #0: subdevice #0 We can see that there are two \"cards\" in the system. Card 0 is dedicated to HDMI output. Card 1 provides audio output to the speakers and headphones of the laptop and can capture from the microphone. Aside: In ALSA, a hardware card/device combo is referred to with name in a special format. \"hw:%d,%d\" So to refer to our built-in audio card, we would use \"hw:1,0\". If you plug a USB device into your computer, you can see a new \"card\" appear. Here, we've plugged in a USB playback and capture device. Using aplay -l , we see the following new device. $ aplay -l ... card 3: Device [USB Audio Device], device 0: USB Audio [USB Audio] Subdevices: 1/1 Subdevice #0: subdevice #0 Each of these sound devices has a \"device name\" and a \"display name\" in the Alsa Sound Kit. For these hardward devices, the \"device name\" is its card/device combo referred to in a special format: \"hw:%d,%d\" . The \"display name\" of the device is the more human-friendly string at the end of the line in square brackets. In our system, we have the following pairs. Device Name Display Name hw:0,0 HDMI 0 hw:0,1 HDMI 0 hw:0,2 HDMI 0 hw:0,3 HDMI 0 hw:1,0 CS4208 Analog hw:1,1 CS4208 Digital hw:3,0 USB Audio Other (pseudo) Devices ALSA provides a plug-in architecture that allows for virtual PCM devices. Some of these devices can be used to playback or capture sounds, others process or modify PCM (waveform) buffers. You can see a list of them using aplay -L . The output of this command does not make it apparent which of these devices can be used for playback or capture: that information is embedded in the ALSA system and is available through C functions. Fortunately, the Alsa Sound Kit is aware of how to find these. For these pseudo-devices, their \"device name\" (the name used in code) is the same as their \"display name.\" On our system, the following pseudo-devices are available. Device Name Display Name default default null null pulse pulse A Linux Audio system is usually configured with the \"default\" and \"null\" pseudo-devices available. The \"default\" device corresponds to whatever is selected as the desktop speaker and microphone in a standard setup. The \"null\" device is a special device that drops all samples sent to it, and produces \"zeroes\" when used as a capture device. Our system alsa has a pseudo-device for \"pulse\" - the \"Pulse Sound Server\". Pulse and JACK Pulse and JACK are both audio sound servers. Each of these \"takes over\" a hardware device and provides a virtual interface for application programs to use. JACK is used in high-performance audio environments and provides an \"audio-bus\" abstraction for sharing low-latency audio streams between applications. JACK is not configured by default on many Linux systems, but is easy to install. Programs that make the optimal use of JACK use the JACK API directly rather than ALSA. (Note: JACK can also provide access via an ALSA Virtual PCM.) Pulse is used on the desktop to multiplex the one selected hardware audio device between all of the applications on the desktop. ALSA applications that open the \"default\" or \"pulse\" audio device on a Pulse-enabled system actually open a port to the Pulse sound server, which then mixes its sound output with other applications that are producing sounds at the same time. Pulse makes applications more friendly on the desktop, but adds a little bit of latency. If your target application is a realtime synthesizer, for instance, it is advisable to bypass Pulse and access a hardware (non-virtual) device directly. Note: Pulse and pasuspender When an audio device is in use by Pulse, it is \"busy\" for other applications. If you need to temporarily pause Pulse and make it release a device, use the pasuspender command. $ pasuspender -- pgm args ... ASK Pcm An ASKPcm object is the Alsa Sound Kit wrapper for an ALSA Pcm handle. The ASKPcm interface provides methods for: opening a PCM querying its capabilities configuring its parameters registering audio callback blocks starting and stopping the audio thread This section will describe how to open and configure an audio device, and how to play a sound. Open a PCM The initializer for an ASKPcm opens the named ALSA device for playback or capture. The ALSA \"device name\" should be specified. NSError *error; ASKPcm *pcm = [[ASKPcm alloc] initWithName:@\"default\" stream:SND_PCM_STREAM_PLAYBACK // or SND_PCM_STREAM_CAPTURE error:&error]; If the PCM cannot be opened because then name doesn't exist, or because it is in use by another program, or because it is not usable for the \"stream\" specified there will be an error returned. Configure a PCM Once a PCM is opened, it must be configured for use. This involves setting two sets of parameters: the Hardware Parameters, and the Software Parameters. The Hardware Parameters necessary include the sample rate, the number of channels (stereo or mono), the memory layout of the samples (interleaved or noninterleaved) and the format (16-bit unsigned integer, 32-bit signed, float, etc) of the samples. It is also necessary to set the period size and number of periods, or equivalently the total buffer size and number of periods. What is the period size? The period size is the number of samples transferred to an audio device as a unit. A user program needs to be able to write new samples to a memory area different than that being transferred, so there must be at least room for at least two periods worth of samples in the buffer. But in some applications it may be useful to size the buffer large enough to hold three or four periods of samples. These parameters are the \"period size\" and \"number of periods.\" The total \"buffer size\" equals the \"period size\" multiplied by the \"number of periods.\" How Readers and Writers share the Audio Buffer Whether your PCM device is a PLAYBACK or a CAPTURE device, there is a circular buffer between your code and the hardware. In the case of PLAYBACK, your audio thread is writing samples to the buffer, and the hardware device is reading samples from the buffer. The figure below illustrates the relationship. The choice of hardware parameters and your algorithm design must all cooperate for this to work smoothly. If the software cannot produce new samples at the rate the hardware is consuming them, then a condition known as an \"underrun\" occurs. If the software is designed to efficiently produce large chunks of audio, this may reduce the occurrence of underruns, but it can increase the total latency of the system. Balancing the parameters with the algorithm design of your software is an art in itself. Set the Hardware Parameters The Hardware Parameters are described by an ASKPcmHwParams object. A HW Params object describes a space of configuration variables that the device might support. For each of the Hardware Parameters, the ASKPcmHwParams holds a minimum and maximum value that might be satisfied by the device. To set the Hardware Parameters of a PCM you must first get its ASKPcmHwParams with the following method call. This object can be inspected by printing it. ASKPcmHwParams *hwparams = [pcm getHwParams:&error]; NSLog(@\"%@); On our laptop, if we open PCM named \"hw:1,0\" (the Intel CS4208 Analog device), we see the following output. For each parameter, the currently selected value is followed by its allowed range in parentheses. A \"U\" means the current value is unspecified. 2020-12-29 14:17:38.025 miniosc1[104007:104007] ALSA-HWPARAMS chan:U(2,4) rate:U(32000,192000), period:U(8,8192), periods:U(2,32), bufsize:U(16,16384) buftime:U(83,512000) access:<U>:(\"MMAP_INTERLEAVED\", \"RW_INTERLEAVED\") format:<U>:(\"S16_LE\", \"S32_LE\") This device supports the following Hardware Parameters ranges: it supports from two to four channels a sample rate between 32000 and 192000 samples per second a period size of 8 to 8192 the number of periods from 2 to 32 a buffersize from 16 to 16384 (buftime is the bufsize divided by sample rate) an Interleaved memory access pattern and signed 16-bit or 32-bit little endian integers That's a lot of information! And it turns out that many combinations of values allowed by the ranges do not necessarily work. Very tiny periods stress the CPU beyond what it can deliver. (Note: if experimenting with your computer, trying a very low period size may hang your system!) Period sizes that are too large limit the total number of periods. In any event, one by one, we must set all of these parameters to something allowed. For instance, we might set the sample rate first to 44100. ok = [pcm setRate:hwparams val:44100 error:&error]; if (ok == NO) { NSLog(@\"Error setting rate:%@\", error); exit(1); } NSLog(@\"%@\", hwparams); If we did, and we printed out the new HW Params object, it would look like the following. The current value of the rate parameter is 44100 instead of \"U\". 2020-12-29 14:27:54.640 miniosc1[104338:104338] ALSA-HWPARAMS chan:U(2,4) rate:44100(44100,44100), period:U(8,8192), periods:U(2,32), bufsize:U(16,16384) buftime:U(362,371520) access:<U>:(\"MMAP_INTERLEAVED\", \"RW_INTERLEAVED\") format:<U>:(\"S16_LE\", \"S32_LE\") Our example program examples-ask/miniosc.m shows how to set the Hardware Parameters in turn. The order our program sets them is this. set sample rate set number of channels (2) set access pattern (INTERLEAVED) set format to S32_LE set the number of periods as near to 2 as possible set the period size to 1024 samples Once the ASKPcmHwParams object has been configured, the PCM is set to use these values. // Now set the HW params ok = [pcm setHwParams:hwparams error:&error]; if (ok == NO) { NSLog(@\"Could not set hw params:%@\", error); exit(1); } Set the Software Parameters The Software Parameters describe some of the characteristics of how the buffer of samples is transferred to the audio device. These follow the same pattern of first getting the current Software Paramters, adjusting the values and setting the parameters. The following works for most devices. // Set Software Parameters ASKPcmSwParams *swparams = [pcm getSwParams]; ok = [pcm setAvailMin:swparams val:persize error:&error]; if (ok == NO) { NSLog(@\"Error setting avail min:%@\", error); exit(1); } ok = [pcm setStartThreshold:swparams val:0 error:&error]; if (ok == NO) { NSLog(@\"Error setting start threshold:%@\", error); exit(1); } ok = [pcm setSwParams:swparams error:&error]; if (ok == NO) { NSLog(@\"Could not set sw params:%@\", error); exit(1); } The ASKPcmSwParams object can be inspected too. It looks like this for the device we opened earlier. 2020-12-29 14:41:47.412 miniosc1[104884:104884] ALSA-SWPARAMS tstampmode:NONE amin:1024 per:0 start:1 stop:2048 sil:0 size:0 Set the Callbacks An ASKPcm manages the audio thread and collects samples for playing (or capturing) through callback blocks. For a playback PCM, the block is defined with the onPlayback: method. The block specified must produce one period's worth of frames each time it is called (the parameter nframes will be the period size). [pcm onPlayback:^(snd_pcm_sframes_t nframes) { return (void*) wav; }]; The block must return the samples of the period as a void* . The samples must match the \"format\" and \"access\" specified when setting the Hardware parameters. After the samples are written to the PCM, a second block is called. This block can be used by user-code for maintenance operations. [pcm onPlaybackCleanup:^{ // ... do something after the samples were transferred to the PCM }] There is also a callback block that can receive an error. The value passed to the block is the ALSA error code. [pcm onPlaybackThreadError:^(int err) { NSLog(@\"Got Thread Error:%d\", err); exit(1); }]; Capture Callbacks For a capture PCM, there are two callbacks. The first must return a pointer to a memory area big enough to hold a periods worth of samples. The second is called after the samples have been written there. [pcm onCaptureBufferBlock:^{ return (void*) wav; }] [pcm onCapture:^(snd_pcm_sframes_t nframes) { // ... handle the frames written to *wav }] There is also a callback block that can receive an error. The value passed to the block is the ALSA error code. [pcm onCaptureThreadError:^(int err) { NSLog(@\"Got Thread Error:%d\", err); exit(1); }]; An example oscilator: A440 The example program in examples-ask/miniosc1.m sets up a PCM and plays a SINE-wave note: an A440. The code setting up the sample buffer and filling it with samples is shown below. There are some interesting things coded here that point to some of the tedium of working with PCM devices: many of the obtained hardware parameters place requirements on the C code written. the frequency increment ( dphi ) is a function of sample rate the size of the data buffer is dependent not only on period size, but sample format ( int32_t ) the access pattern (interleaved) is reflected in the loop // Create a waveform: A440 __block double phi = 0; double dphi = (2 * M_PI) * 440.0 / 22050.0; NSData *data = [NSMutableData dataWithLength:(2 * persize * sizeof(int32_t))]; int32_t *wav = (int32_t*) [data bytes]; // Install callback [pcm onPlayback:^(snd_pcm_sframes_t nframes) { for (int i = 0; i < 1024; i++) { double sound = sin(phi) * (1 << 24); wav[i*2] = sound; wav[i*2 + 1] = sound; phi += dphi; if (phi >= 2*M_PI) { phi -= 2*M_PI; } } return (void*) wav; }]; The use of the block as a callback lends some conveniences however. The variables and memory buffer used inside the block are \"captured\" by the compiler. Without blocks, an equivalent callback function would need more arguments. Blocks make the callback from the audio thread easier to write. Rules on the Callback Blocks The ASKPcm sets up and runs the audio thread. The callback blocks execute in the context of the audio thread. You must be very careful about what operations your code performs in these blocks. An operation that could cause the code to \"block\" can (will!) create audio artifacts! Do not perform memory allocation. Do not read or write files. Do not use locks. The reason that the blocks are alright is that they are allocated before the audio thread starts playing them. A great article about some common pitfalls of audio thread programming is this: Four common mistakes in audio development by Michael Tyson Another great article is this one by Ross Bencina: Real-time audio programming 101: time waits for nothing Start the Audio Thread After the ASKPcm is configured and the appropriate callback blocks are set, the PCM can be launched. // Launch the PCM Thread ok = [pcm startThreadWithError:&error]; if (ok == NO) { NSLog(@\"Could not start PCM thread:%@\", error); exit(1); } If there is no error, the audio thread will play until stopped. Experiment with ALSA Devices The example program examples-ask/miniosc1.m is a small program that shows all of the steps necessary to open a PCM device and play a tone. The program accepts an \"device name\" as an argument. Try playing some of the different devices on your system. $ ./miniosc1 default $ ./miniosc1 hw:1,0 $ ./miniosc1 hw:3,0 You may find that some devices are silent, or that the parameters do not configure that device appropriately. See if you can fix the parameters, format size or access to make the device play. Listing Available PCMs The Alsa Sound Kit provides the ASKPcmList utlity class for listing the PCMs in your system. ASKPcmList *list = [[ASKPcmList alloc] initWithStream:SND_PCM_STREAM_PLAYBACK]; for (ASKPcmListItem *item in list.pcmitems) { NSLog(@\"device-name:%@ display-name:%@\", item.pcmDeviceName, item.pcmDisplayName); } On our system, this produces the following output. 2020-12-30 09:17:32.146 askpcmlist[112073:112073] device-name:hw:0,3 display-name:HDMI 0 2020-12-30 09:17:32.146 askpcmlist[112073:112073] device-name:hw:0,7 display-name:HDMI 1 2020-12-30 09:17:32.146 askpcmlist[112073:112073] device-name:hw:0,8 display-name:HDMI 2 2020-12-30 09:17:32.146 askpcmlist[112073:112073] device-name:hw:0,9 display-name:HDMI 3 2020-12-30 09:17:32.146 askpcmlist[112073:112073] device-name:hw:0,10 display-name:HDMI 4 2020-12-30 09:17:32.146 askpcmlist[112073:112073] device-name:hw:1,0 display-name:CS4208 Analog 2020-12-30 09:17:32.146 askpcmlist[112073:112073] device-name:hw:1,1 display-name:CS4208 Digital 2020-12-30 09:17:32.146 askpcmlist[112073:112073] device-name:hw:3,0 display-name:USB Audio 2020-12-30 09:17:32.146 askpcmlist[112073:112073] device-name:default display-name:default 2020-12-30 09:17:32.146 askpcmlist[112073:112073] device-name:null display-name:null 2020-12-30 09:17:32.146 askpcmlist[112073:112073] device-name:pulse display-name:pulse Summary This chapter described ALSA sound devices called \"PCMs\". We gave some examples of how to examine and play the PCMs in your Linux system by using the aplay command. The Alsa Sound Kit provides an ASKPcm object for interfacing playback and capture PCMs. It provides methods for configuring the PCM Hardware and Software parameters. The ASKPcm class also manages the audio thread and recovers from over/underrun conditions, allowing the audio programmer to focus on just the callbacks that provide playback or gather capture data. The ASKSeq interface is a thin wrapper around the ALSA PCM handle itself. It does not hide the problems of managing memory and layout for the different access types and format sizes. For that we will introduce the MSKContext as part of the McLaren Synth Kit in the next chapters.","title":"Alsa Sound Kit - PCM"},{"location":"ask-pcm/#alsa-sound-kit-pcm","text":"The Alsa Sound Kit (ASK) provides Objective-C wrappers around ALSA MIDI devices and ALSA PCM (sound) devices. This chapter describes how sound devices are configured and how sounds are played or captured.","title":"Alsa Sound Kit - PCM"},{"location":"ask-pcm/#alsa-pcm-system-overview","text":"The Advance Linux Sound Architecture (ALSA) library calls a device that can translate continuous sound waves into a series of samples a \"PCM\" (PCM stands for Pulse Code Modulation). ALSA PCM devices can PLAY and CAPTURE sounds. In your system, a sound capability is provided by a \"card.\" These days, a sound card is usually integrated directly into your laptop motherboard. When you plug a USB audio device into your system, that will appear as a card too. A card can have one or more devices. Some of the devices can play or capture audio, and there can be devices for MIDI or other functions too. For our purposes here, we'll be focusing just on the devices that play or capture audio. On a Linux computer, you can list sound devices using the aplay command. This command can also play audio samples. Try it out like this. $ aplay /usr/share/sounds/alsa/Front_Center.wav Playing WAVE '/usr/share/sounds/alsa/Front_Center.wav' : Signed 16 bit Little Endian, Rate 48000 Hz, Mono If all goes well, you'll hear a sound saying \"Front. Center.\" and you'll see the output above. To list the audio cards and devices in your system use aplay -l . On our laptop at McLaren Labs, it produces the following output. $ aplay -l **** List of PLAYBACK Hardware Devices **** card 0: HDMI [HDA Intel HDMI], device 3: HDMI 0 [HDMI 0] Subdevices: 1/1 Subdevice #0: subdevice #0 card 0: HDMI [HDA Intel HDMI], device 7: HDMI 1 [HDMI 1] Subdevices: 1/1 Subdevice #0: subdevice #0 card 0: HDMI [HDA Intel HDMI], device 8: HDMI 2 [HDMI 2] Subdevices: 1/1 Subdevice #0: subdevice #0 card 0: HDMI [HDA Intel HDMI], device 9: HDMI 3 [HDMI 3] Subdevices: 1/1 Subdevice #0: subdevice #0 card 0: HDMI [HDA Intel HDMI], device 10: HDMI 4 [HDMI 4] Subdevices: 1/1 Subdevice #0: subdevice #0 card 1: PCH [HDA Intel PCH], device 0: CS4208 Analog [CS4208 Analog] Subdevices: 1/1 Subdevice #0: subdevice #0 card 1: PCH [HDA Intel PCH], device 1: CS4208 Digital [CS4208 Digital] Subdevices: 1/1 Subdevice #0: subdevice #0 We can see that there are two \"cards\" in the system. Card 0 is dedicated to HDMI output. Card 1 provides audio output to the speakers and headphones of the laptop and can capture from the microphone. Aside: In ALSA, a hardware card/device combo is referred to with name in a special format. \"hw:%d,%d\" So to refer to our built-in audio card, we would use \"hw:1,0\". If you plug a USB device into your computer, you can see a new \"card\" appear. Here, we've plugged in a USB playback and capture device. Using aplay -l , we see the following new device. $ aplay -l ... card 3: Device [USB Audio Device], device 0: USB Audio [USB Audio] Subdevices: 1/1 Subdevice #0: subdevice #0 Each of these sound devices has a \"device name\" and a \"display name\" in the Alsa Sound Kit. For these hardward devices, the \"device name\" is its card/device combo referred to in a special format: \"hw:%d,%d\" . The \"display name\" of the device is the more human-friendly string at the end of the line in square brackets. In our system, we have the following pairs. Device Name Display Name hw:0,0 HDMI 0 hw:0,1 HDMI 0 hw:0,2 HDMI 0 hw:0,3 HDMI 0 hw:1,0 CS4208 Analog hw:1,1 CS4208 Digital hw:3,0 USB Audio","title":"ALSA PCM System Overview"},{"location":"ask-pcm/#other-pseudo-devices","text":"ALSA provides a plug-in architecture that allows for virtual PCM devices. Some of these devices can be used to playback or capture sounds, others process or modify PCM (waveform) buffers. You can see a list of them using aplay -L . The output of this command does not make it apparent which of these devices can be used for playback or capture: that information is embedded in the ALSA system and is available through C functions. Fortunately, the Alsa Sound Kit is aware of how to find these. For these pseudo-devices, their \"device name\" (the name used in code) is the same as their \"display name.\" On our system, the following pseudo-devices are available. Device Name Display Name default default null null pulse pulse A Linux Audio system is usually configured with the \"default\" and \"null\" pseudo-devices available. The \"default\" device corresponds to whatever is selected as the desktop speaker and microphone in a standard setup. The \"null\" device is a special device that drops all samples sent to it, and produces \"zeroes\" when used as a capture device. Our system alsa has a pseudo-device for \"pulse\" - the \"Pulse Sound Server\".","title":"Other (pseudo) Devices"},{"location":"ask-pcm/#pulse-and-jack","text":"Pulse and JACK are both audio sound servers. Each of these \"takes over\" a hardware device and provides a virtual interface for application programs to use. JACK is used in high-performance audio environments and provides an \"audio-bus\" abstraction for sharing low-latency audio streams between applications. JACK is not configured by default on many Linux systems, but is easy to install. Programs that make the optimal use of JACK use the JACK API directly rather than ALSA. (Note: JACK can also provide access via an ALSA Virtual PCM.) Pulse is used on the desktop to multiplex the one selected hardware audio device between all of the applications on the desktop. ALSA applications that open the \"default\" or \"pulse\" audio device on a Pulse-enabled system actually open a port to the Pulse sound server, which then mixes its sound output with other applications that are producing sounds at the same time. Pulse makes applications more friendly on the desktop, but adds a little bit of latency. If your target application is a realtime synthesizer, for instance, it is advisable to bypass Pulse and access a hardware (non-virtual) device directly. Note: Pulse and pasuspender When an audio device is in use by Pulse, it is \"busy\" for other applications. If you need to temporarily pause Pulse and make it release a device, use the pasuspender command. $ pasuspender -- pgm args ...","title":"Pulse and JACK"},{"location":"ask-pcm/#ask-pcm","text":"An ASKPcm object is the Alsa Sound Kit wrapper for an ALSA Pcm handle. The ASKPcm interface provides methods for: opening a PCM querying its capabilities configuring its parameters registering audio callback blocks starting and stopping the audio thread This section will describe how to open and configure an audio device, and how to play a sound.","title":"ASK Pcm"},{"location":"ask-pcm/#open-a-pcm","text":"The initializer for an ASKPcm opens the named ALSA device for playback or capture. The ALSA \"device name\" should be specified. NSError *error; ASKPcm *pcm = [[ASKPcm alloc] initWithName:@\"default\" stream:SND_PCM_STREAM_PLAYBACK // or SND_PCM_STREAM_CAPTURE error:&error]; If the PCM cannot be opened because then name doesn't exist, or because it is in use by another program, or because it is not usable for the \"stream\" specified there will be an error returned.","title":"Open a PCM"},{"location":"ask-pcm/#configure-a-pcm","text":"Once a PCM is opened, it must be configured for use. This involves setting two sets of parameters: the Hardware Parameters, and the Software Parameters. The Hardware Parameters necessary include the sample rate, the number of channels (stereo or mono), the memory layout of the samples (interleaved or noninterleaved) and the format (16-bit unsigned integer, 32-bit signed, float, etc) of the samples. It is also necessary to set the period size and number of periods, or equivalently the total buffer size and number of periods.","title":"Configure a PCM"},{"location":"ask-pcm/#what-is-the-period-size","text":"The period size is the number of samples transferred to an audio device as a unit. A user program needs to be able to write new samples to a memory area different than that being transferred, so there must be at least room for at least two periods worth of samples in the buffer. But in some applications it may be useful to size the buffer large enough to hold three or four periods of samples. These parameters are the \"period size\" and \"number of periods.\" The total \"buffer size\" equals the \"period size\" multiplied by the \"number of periods.\"","title":"What is the period size?"},{"location":"ask-pcm/#how-readers-and-writers-share-the-audio-buffer","text":"Whether your PCM device is a PLAYBACK or a CAPTURE device, there is a circular buffer between your code and the hardware. In the case of PLAYBACK, your audio thread is writing samples to the buffer, and the hardware device is reading samples from the buffer. The figure below illustrates the relationship. The choice of hardware parameters and your algorithm design must all cooperate for this to work smoothly. If the software cannot produce new samples at the rate the hardware is consuming them, then a condition known as an \"underrun\" occurs. If the software is designed to efficiently produce large chunks of audio, this may reduce the occurrence of underruns, but it can increase the total latency of the system. Balancing the parameters with the algorithm design of your software is an art in itself.","title":"How Readers and Writers share the Audio Buffer"},{"location":"ask-pcm/#set-the-hardware-parameters","text":"The Hardware Parameters are described by an ASKPcmHwParams object. A HW Params object describes a space of configuration variables that the device might support. For each of the Hardware Parameters, the ASKPcmHwParams holds a minimum and maximum value that might be satisfied by the device. To set the Hardware Parameters of a PCM you must first get its ASKPcmHwParams with the following method call. This object can be inspected by printing it. ASKPcmHwParams *hwparams = [pcm getHwParams:&error]; NSLog(@\"%@); On our laptop, if we open PCM named \"hw:1,0\" (the Intel CS4208 Analog device), we see the following output. For each parameter, the currently selected value is followed by its allowed range in parentheses. A \"U\" means the current value is unspecified. 2020-12-29 14:17:38.025 miniosc1[104007:104007] ALSA-HWPARAMS chan:U(2,4) rate:U(32000,192000), period:U(8,8192), periods:U(2,32), bufsize:U(16,16384) buftime:U(83,512000) access:<U>:(\"MMAP_INTERLEAVED\", \"RW_INTERLEAVED\") format:<U>:(\"S16_LE\", \"S32_LE\") This device supports the following Hardware Parameters ranges: it supports from two to four channels a sample rate between 32000 and 192000 samples per second a period size of 8 to 8192 the number of periods from 2 to 32 a buffersize from 16 to 16384 (buftime is the bufsize divided by sample rate) an Interleaved memory access pattern and signed 16-bit or 32-bit little endian integers That's a lot of information! And it turns out that many combinations of values allowed by the ranges do not necessarily work. Very tiny periods stress the CPU beyond what it can deliver. (Note: if experimenting with your computer, trying a very low period size may hang your system!) Period sizes that are too large limit the total number of periods. In any event, one by one, we must set all of these parameters to something allowed. For instance, we might set the sample rate first to 44100. ok = [pcm setRate:hwparams val:44100 error:&error]; if (ok == NO) { NSLog(@\"Error setting rate:%@\", error); exit(1); } NSLog(@\"%@\", hwparams); If we did, and we printed out the new HW Params object, it would look like the following. The current value of the rate parameter is 44100 instead of \"U\". 2020-12-29 14:27:54.640 miniosc1[104338:104338] ALSA-HWPARAMS chan:U(2,4) rate:44100(44100,44100), period:U(8,8192), periods:U(2,32), bufsize:U(16,16384) buftime:U(362,371520) access:<U>:(\"MMAP_INTERLEAVED\", \"RW_INTERLEAVED\") format:<U>:(\"S16_LE\", \"S32_LE\") Our example program examples-ask/miniosc.m shows how to set the Hardware Parameters in turn. The order our program sets them is this. set sample rate set number of channels (2) set access pattern (INTERLEAVED) set format to S32_LE set the number of periods as near to 2 as possible set the period size to 1024 samples Once the ASKPcmHwParams object has been configured, the PCM is set to use these values. // Now set the HW params ok = [pcm setHwParams:hwparams error:&error]; if (ok == NO) { NSLog(@\"Could not set hw params:%@\", error); exit(1); }","title":"Set the Hardware Parameters"},{"location":"ask-pcm/#set-the-software-parameters","text":"The Software Parameters describe some of the characteristics of how the buffer of samples is transferred to the audio device. These follow the same pattern of first getting the current Software Paramters, adjusting the values and setting the parameters. The following works for most devices. // Set Software Parameters ASKPcmSwParams *swparams = [pcm getSwParams]; ok = [pcm setAvailMin:swparams val:persize error:&error]; if (ok == NO) { NSLog(@\"Error setting avail min:%@\", error); exit(1); } ok = [pcm setStartThreshold:swparams val:0 error:&error]; if (ok == NO) { NSLog(@\"Error setting start threshold:%@\", error); exit(1); } ok = [pcm setSwParams:swparams error:&error]; if (ok == NO) { NSLog(@\"Could not set sw params:%@\", error); exit(1); } The ASKPcmSwParams object can be inspected too. It looks like this for the device we opened earlier. 2020-12-29 14:41:47.412 miniosc1[104884:104884] ALSA-SWPARAMS tstampmode:NONE amin:1024 per:0 start:1 stop:2048 sil:0 size:0","title":"Set the Software Parameters"},{"location":"ask-pcm/#set-the-callbacks","text":"An ASKPcm manages the audio thread and collects samples for playing (or capturing) through callback blocks. For a playback PCM, the block is defined with the onPlayback: method. The block specified must produce one period's worth of frames each time it is called (the parameter nframes will be the period size). [pcm onPlayback:^(snd_pcm_sframes_t nframes) { return (void*) wav; }]; The block must return the samples of the period as a void* . The samples must match the \"format\" and \"access\" specified when setting the Hardware parameters. After the samples are written to the PCM, a second block is called. This block can be used by user-code for maintenance operations. [pcm onPlaybackCleanup:^{ // ... do something after the samples were transferred to the PCM }] There is also a callback block that can receive an error. The value passed to the block is the ALSA error code. [pcm onPlaybackThreadError:^(int err) { NSLog(@\"Got Thread Error:%d\", err); exit(1); }];","title":"Set the Callbacks"},{"location":"ask-pcm/#capture-callbacks","text":"For a capture PCM, there are two callbacks. The first must return a pointer to a memory area big enough to hold a periods worth of samples. The second is called after the samples have been written there. [pcm onCaptureBufferBlock:^{ return (void*) wav; }] [pcm onCapture:^(snd_pcm_sframes_t nframes) { // ... handle the frames written to *wav }] There is also a callback block that can receive an error. The value passed to the block is the ALSA error code. [pcm onCaptureThreadError:^(int err) { NSLog(@\"Got Thread Error:%d\", err); exit(1); }];","title":"Capture Callbacks"},{"location":"ask-pcm/#an-example-oscilator-a440","text":"The example program in examples-ask/miniosc1.m sets up a PCM and plays a SINE-wave note: an A440. The code setting up the sample buffer and filling it with samples is shown below. There are some interesting things coded here that point to some of the tedium of working with PCM devices: many of the obtained hardware parameters place requirements on the C code written. the frequency increment ( dphi ) is a function of sample rate the size of the data buffer is dependent not only on period size, but sample format ( int32_t ) the access pattern (interleaved) is reflected in the loop // Create a waveform: A440 __block double phi = 0; double dphi = (2 * M_PI) * 440.0 / 22050.0; NSData *data = [NSMutableData dataWithLength:(2 * persize * sizeof(int32_t))]; int32_t *wav = (int32_t*) [data bytes]; // Install callback [pcm onPlayback:^(snd_pcm_sframes_t nframes) { for (int i = 0; i < 1024; i++) { double sound = sin(phi) * (1 << 24); wav[i*2] = sound; wav[i*2 + 1] = sound; phi += dphi; if (phi >= 2*M_PI) { phi -= 2*M_PI; } } return (void*) wav; }]; The use of the block as a callback lends some conveniences however. The variables and memory buffer used inside the block are \"captured\" by the compiler. Without blocks, an equivalent callback function would need more arguments. Blocks make the callback from the audio thread easier to write.","title":"An example oscilator: A440"},{"location":"ask-pcm/#rules-on-the-callback-blocks","text":"The ASKPcm sets up and runs the audio thread. The callback blocks execute in the context of the audio thread. You must be very careful about what operations your code performs in these blocks. An operation that could cause the code to \"block\" can (will!) create audio artifacts! Do not perform memory allocation. Do not read or write files. Do not use locks. The reason that the blocks are alright is that they are allocated before the audio thread starts playing them. A great article about some common pitfalls of audio thread programming is this: Four common mistakes in audio development by Michael Tyson Another great article is this one by Ross Bencina: Real-time audio programming 101: time waits for nothing","title":"Rules on the Callback Blocks"},{"location":"ask-pcm/#start-the-audio-thread","text":"After the ASKPcm is configured and the appropriate callback blocks are set, the PCM can be launched. // Launch the PCM Thread ok = [pcm startThreadWithError:&error]; if (ok == NO) { NSLog(@\"Could not start PCM thread:%@\", error); exit(1); } If there is no error, the audio thread will play until stopped.","title":"Start the Audio Thread"},{"location":"ask-pcm/#experiment-with-alsa-devices","text":"The example program examples-ask/miniosc1.m is a small program that shows all of the steps necessary to open a PCM device and play a tone. The program accepts an \"device name\" as an argument. Try playing some of the different devices on your system. $ ./miniosc1 default $ ./miniosc1 hw:1,0 $ ./miniosc1 hw:3,0 You may find that some devices are silent, or that the parameters do not configure that device appropriately. See if you can fix the parameters, format size or access to make the device play.","title":"Experiment with ALSA Devices"},{"location":"ask-pcm/#listing-available-pcms","text":"The Alsa Sound Kit provides the ASKPcmList utlity class for listing the PCMs in your system. ASKPcmList *list = [[ASKPcmList alloc] initWithStream:SND_PCM_STREAM_PLAYBACK]; for (ASKPcmListItem *item in list.pcmitems) { NSLog(@\"device-name:%@ display-name:%@\", item.pcmDeviceName, item.pcmDisplayName); } On our system, this produces the following output. 2020-12-30 09:17:32.146 askpcmlist[112073:112073] device-name:hw:0,3 display-name:HDMI 0 2020-12-30 09:17:32.146 askpcmlist[112073:112073] device-name:hw:0,7 display-name:HDMI 1 2020-12-30 09:17:32.146 askpcmlist[112073:112073] device-name:hw:0,8 display-name:HDMI 2 2020-12-30 09:17:32.146 askpcmlist[112073:112073] device-name:hw:0,9 display-name:HDMI 3 2020-12-30 09:17:32.146 askpcmlist[112073:112073] device-name:hw:0,10 display-name:HDMI 4 2020-12-30 09:17:32.146 askpcmlist[112073:112073] device-name:hw:1,0 display-name:CS4208 Analog 2020-12-30 09:17:32.146 askpcmlist[112073:112073] device-name:hw:1,1 display-name:CS4208 Digital 2020-12-30 09:17:32.146 askpcmlist[112073:112073] device-name:hw:3,0 display-name:USB Audio 2020-12-30 09:17:32.146 askpcmlist[112073:112073] device-name:default display-name:default 2020-12-30 09:17:32.146 askpcmlist[112073:112073] device-name:null display-name:null 2020-12-30 09:17:32.146 askpcmlist[112073:112073] device-name:pulse display-name:pulse","title":"Listing Available PCMs"},{"location":"ask-pcm/#summary","text":"This chapter described ALSA sound devices called \"PCMs\". We gave some examples of how to examine and play the PCMs in your Linux system by using the aplay command. The Alsa Sound Kit provides an ASKPcm object for interfacing playback and capture PCMs. It provides methods for configuring the PCM Hardware and Software parameters. The ASKPcm class also manages the audio thread and recovers from over/underrun conditions, allowing the audio programmer to focus on just the callbacks that provide playback or gather capture data. The ASKSeq interface is a thin wrapper around the ALSA PCM handle itself. It does not hide the problems of managing memory and layout for the different access types and format sizes. For that we will introduce the MSKContext as part of the McLaren Synth Kit in the next chapters.","title":"Summary"},{"location":"conclusion/","text":"Conclusion This project describes the McLaren Synth Kit. It consists of two parts: a lower-level library called the Alsa Sound Kit that provides a slim Objective-C wrapper around the MIDI and PCM components of the ALSA sound library. THe Mclaren Synth Kit builds on the Alsa Sound Kit to provide a high-level abstraction of audio synthesis as the construction of an audio graph that is evaulated to produce sounds. The Alsa Sound Kit does not attempt to hide the details of the ALSA sound library. Rather, it aims to smooth the interface between the low-level C library and a slightly higher-level abstraction of Objective-C with dispatch queues. For MIDI, we map an ALSA SEQ interface directly to a dispatch source operating with a high-priority queue. This maps the MIDI system into the dispatch queue interface used pervasively in Objective-C. ALSA PCMs are mapped to a high-priority thread with Objective-C callback \"blocks.\" Blocks help to simplify some aspects of writing callback functions. The most abstract interface are the Context and Voice classes of the Mclaren SYnth Kit. In this level of abstraction, the callbacks of the PCM layer are completely hidden and audio synthesis is expressed as a series of operators Voices. The libraries have been in use and are demonstrated by the softare projects released by McLaren Labs . Future installments of this project will describe further aspects of the libraries through examples and explations. As always, the examples here are designed to be compiled and run and to form the basis of your own audio explorations.","title":"Conclusion"},{"location":"conclusion/#conclusion","text":"This project describes the McLaren Synth Kit. It consists of two parts: a lower-level library called the Alsa Sound Kit that provides a slim Objective-C wrapper around the MIDI and PCM components of the ALSA sound library. THe Mclaren Synth Kit builds on the Alsa Sound Kit to provide a high-level abstraction of audio synthesis as the construction of an audio graph that is evaulated to produce sounds. The Alsa Sound Kit does not attempt to hide the details of the ALSA sound library. Rather, it aims to smooth the interface between the low-level C library and a slightly higher-level abstraction of Objective-C with dispatch queues. For MIDI, we map an ALSA SEQ interface directly to a dispatch source operating with a high-priority queue. This maps the MIDI system into the dispatch queue interface used pervasively in Objective-C. ALSA PCMs are mapped to a high-priority thread with Objective-C callback \"blocks.\" Blocks help to simplify some aspects of writing callback functions. The most abstract interface are the Context and Voice classes of the Mclaren SYnth Kit. In this level of abstraction, the callbacks of the PCM layer are completely hidden and audio synthesis is expressed as a series of operators Voices. The libraries have been in use and are demonstrated by the softare projects released by McLaren Labs . Future installments of this project will describe further aspects of the libraries through examples and explations. As always, the examples here are designed to be compiled and run and to form the basis of your own audio explorations.","title":"Conclusion"},{"location":"misc/","text":"Miscellaneous Use memlock mlockall(MCL_CURRENT | MCL_FUTURE); http://tedfelix.com/linux/linux-midi.html","title":"Miscellaneous"},{"location":"misc/#miscellaneous","text":"","title":"Miscellaneous"},{"location":"misc/#use-memlock","text":"mlockall(MCL_CURRENT | MCL_FUTURE); http://tedfelix.com/linux/linux-midi.html","title":"Use memlock"},{"location":"msk-extra/","text":"MSK Context Buffer In the Mclaren Synth Kit, all sample buffers are an array of single-precision FLOATs in INTERLEAVED access pattern. An MSKContextBuffer holds an NSData sized to the period size of a configured MSKContext . An MSKContextBuffer is always initialized with a specific Context, and it holds a reference to that Context. ASKContextBuffer *buf = [[ASKContextBuffer alloc] initWithCtx:ctx] If you are writing your own Voices, you will need to access the underlying frames in the _frames instance variable. MSK Context Voice In the Mclaren Synth Kit, an MSKContextVoice is a buffer that has been augmented with callback functions for producing various kinds of sounds or envelopes. The callback functions are called once each period and fill the underlying MSKContextBuffer with samples. The base class MSKContextVoice has a predefined callback that fills the buffer with zeros every time it is updated. In other words, the base MSKContextVoice is a silence generator. Playing a Voice Voices are designed to be played by a Context. To play a Voice, add it to the Context. The following code creates a Voice that plays silence. ASKContextVoice *silence = [[ASKcontextVoice alloc] initWithCtx:ctx] [ctx addVoice:silence] Once added to a Context, a Voice plays until its _active instance variable is set to NO . We could stop our silence voice from playing like this. silence->_active = NO; silence = nil; Part of the logic implemented by the Context is the release of inactive voices ... safely . At some point after the silence voice is marked inactive, the Context will release its reference to the voice. When all of the references to the Voice are released, the memory for the VOice is reclaimed by Objective-C's ARC. The Voice Protocol The example above demonstrates the basis for transferring Voices into and out-of the audio thread. To add a voice to a Context, use the addVoice: method with a Voice. To stop a voice from playing and arrange to have it removed from the Context, cause its _active instance variable to be set to NO . That's it! That is the informal protocol for transferring voices to and from the audio thread. We'll see later that manipulating the _active instance variable is rarely done directly. More usually it is handled by a pre-defined audio unit like an Envelope. Aside: Why is it safe to set the _active instance variable from a thread outside of the audio thread? In the chapter on PCMs, we said that care must be taken to not perform an operation that would block. Reading and writing variables across threads is allowed however. The setting of the _active instance variable to NO from outside the audio thread is allowed. In the callback block of the Context running inside the audio thread, the _active flag is read at the end of each period. Inactive Voices are safely released. MSK Context Envelope There is a formal protocol for Voices that generate an Envelope. Envelopes in the Mclaren Synth Kit implement the standard attack, decay, sustain, release segments. An MSKContextEnvelope implements the following methods. @protocol MSKContextEnvelope - (BOOL) noteOff; - (BOOL) noteAbort; - (BOOL) noteReset:(int)idx; @end Invoking the noteOff method of an Envelope causes it to begin its release. After the release time, the Envelope is marked inactive. Invoking the noteAbort method causes an Envelope to drop to 0.0 by the end of the period. The Envelope is then marked inactive. Invoking noteReset causes an Envelope to start all over. As stated, MSKContextEnvelope is a formal protocol, and not an implementation. The McLaren Synth Kit provides two standard Envelope generators. MSKExpEnvelope - an exponential envelope MSKLinEnvelope - a linear envelope","title":"Msk extra"},{"location":"msk-extra/#msk-context-buffer","text":"In the Mclaren Synth Kit, all sample buffers are an array of single-precision FLOATs in INTERLEAVED access pattern. An MSKContextBuffer holds an NSData sized to the period size of a configured MSKContext . An MSKContextBuffer is always initialized with a specific Context, and it holds a reference to that Context. ASKContextBuffer *buf = [[ASKContextBuffer alloc] initWithCtx:ctx] If you are writing your own Voices, you will need to access the underlying frames in the _frames instance variable.","title":"MSK Context Buffer"},{"location":"msk-extra/#msk-context-voice","text":"In the Mclaren Synth Kit, an MSKContextVoice is a buffer that has been augmented with callback functions for producing various kinds of sounds or envelopes. The callback functions are called once each period and fill the underlying MSKContextBuffer with samples. The base class MSKContextVoice has a predefined callback that fills the buffer with zeros every time it is updated. In other words, the base MSKContextVoice is a silence generator.","title":"MSK Context Voice"},{"location":"msk-extra/#playing-a-voice","text":"Voices are designed to be played by a Context. To play a Voice, add it to the Context. The following code creates a Voice that plays silence. ASKContextVoice *silence = [[ASKcontextVoice alloc] initWithCtx:ctx] [ctx addVoice:silence] Once added to a Context, a Voice plays until its _active instance variable is set to NO . We could stop our silence voice from playing like this. silence->_active = NO; silence = nil; Part of the logic implemented by the Context is the release of inactive voices ... safely . At some point after the silence voice is marked inactive, the Context will release its reference to the voice. When all of the references to the Voice are released, the memory for the VOice is reclaimed by Objective-C's ARC.","title":"Playing a Voice"},{"location":"msk-extra/#the-voice-protocol","text":"The example above demonstrates the basis for transferring Voices into and out-of the audio thread. To add a voice to a Context, use the addVoice: method with a Voice. To stop a voice from playing and arrange to have it removed from the Context, cause its _active instance variable to be set to NO . That's it! That is the informal protocol for transferring voices to and from the audio thread. We'll see later that manipulating the _active instance variable is rarely done directly. More usually it is handled by a pre-defined audio unit like an Envelope. Aside: Why is it safe to set the _active instance variable from a thread outside of the audio thread? In the chapter on PCMs, we said that care must be taken to not perform an operation that would block. Reading and writing variables across threads is allowed however. The setting of the _active instance variable to NO from outside the audio thread is allowed. In the callback block of the Context running inside the audio thread, the _active flag is read at the end of each period. Inactive Voices are safely released.","title":"The Voice Protocol"},{"location":"msk-extra/#msk-context-envelope","text":"There is a formal protocol for Voices that generate an Envelope. Envelopes in the Mclaren Synth Kit implement the standard attack, decay, sustain, release segments. An MSKContextEnvelope implements the following methods. @protocol MSKContextEnvelope - (BOOL) noteOff; - (BOOL) noteAbort; - (BOOL) noteReset:(int)idx; @end Invoking the noteOff method of an Envelope causes it to begin its release. After the release time, the Envelope is marked inactive. Invoking the noteAbort method causes an Envelope to drop to 0.0 by the end of the period. The Envelope is then marked inactive. Invoking noteReset causes an Envelope to start all over. As stated, MSKContextEnvelope is a formal protocol, and not an implementation. The McLaren Synth Kit provides two standard Envelope generators. MSKExpEnvelope - an exponential envelope MSKLinEnvelope - a linear envelope","title":"MSK Context Envelope"},{"location":"msk-over/","text":"McLaren Synth Kit - Overview In the previous chapter, we saw how to work with a PCM to deliver samples to an audio device. Working directly with an audio device exposes many of the hardware parameters to the underlying code. Parameters such as sample rate, period size, access pattern and sample type all must be accounted for. The Mclaren Synth Kit (M.S.K.) presents a level of abstraction above that of the PCM. It has been designed to use Objective-C to design audio generators as a connected graph of low-level audio units called Voices. This chapter describes how to set up and use the standard components of the Mclaren Synth Kit. The MSKContext class is the foundation of the Synth Kit. An MSKContext opens and manages a PCM. It schedules the necessary audio callback blocks and evaluates the voices of the audio graph to generate sounds. An MSKContext also allocates a dispatch queue for timing and for callbacks. As we learned earlier, it is important that the callbacks on the audio thread do not perform blocking operations. The MSKContext and its voices implement a protocol for getting information into and out-of the audio thread. By following the guidelines of this protocol you are ensured to avoid audio glitches or pops. MSK Context The MSKContext does its best to find good hardware and software parameters for the specified PCM device. Opening, configuring and running an MSKContext is straightfoward. The snippet below shows how to open the PCM device named \"default\" for playback. NSString *devName = @\"default\"; NSError *error; // Create an audio context on the 'default' device for playback MSKContext *ctx = [[MSKContext alloc] initWithName:devName andStream:SND_PCM_STREAM_PLAYBACK error:&error]; Next, create a configuration request. The request specifies the sample rate, period size and number of periods. These parameters are key for adjusting the latency of a synthesizer application. // Desired audio context parameters MSKContextRequest *request = [[MSKContextRequest alloc] init]; request.rate = 44000; request.persize = 1024; request.periods = 2; Now, configure the Context with the Request. BOOL ok; // Configure the context with the request ok = [ctx configureForRequest:request error:&error]; If there is no error, the Context has successfully configured the underlying PCM. You can observe the achieved Hardware and Software parameters in the hwparams and swparams property of the Context. NSLog(@\"%@\", ctx.hwparams) NSLog(@\"%@\", ctx.swparams) Aside: the Context exposes many of its internal variables as readonly properties. One of the design decisions behind the Mclaren Synth Kit is that useful implementation variables should be exposed rather than hidden in a private category. Before the Context produces any sound, it needs to be started. This call does two things: it starts the underlying PCM thread, and it launches a dispatch queue timer for monitoring status information coming out of the audio callback. // Start the context ok = [ctx startWithError:&error]; MSK Voice A Voice consists of a sample buffer and some callback functions that fill the buffer with samples. In the Mclaren Synth Kit there are voices that create sounds and voices that control the parameters of other voices. All are children of the MSKContextVoice base class. The compile method of a Voice must be called after it is attached to other objects. A Voice can be played by a Context by adding it to the Context with the addVoice: method. MSKContextVoice *v1 = [[MSKContextVoice alloc] initWithCtx:ctx]; [v1 compile]; [ctx addVoice:v1] It is useful to think about the audio graph constructed by the preceding code. A Voice referenced by the variable v1 has been added to the Context. The Context renders the Voice by reading its samples once every PCM period. The Context invokes a callback function defined by the Voice to create new samples each period. A Voice is rendered by a Context until it is no longer active. (We'll define how Voices become \"inactive\" in a minute.) Voices and Automatic Reference Counting (ARC) A Voice is an Objective-C object and is managed through ARC (Automatic Reference Counting) just like any other object. A Context retains a reference to a Voice as long as it is active. When it is no longer active, the Context releases its reference so the Voice can be reclaimed. Consider the code below. This creates a Voice and hands it to the Context. After the addVoice: call, the variable v1 is set to nil , essentially dropping the reference to the Voice. MSKContextVoice *v1 = [[MSKContextVoice alloc] initWithCtx:ctx]; [v1 compile]; [ctx addVoice:v1] v1 = nil; The Context still holds a reference to the Voice, however. As long as it is \"active\" the reference is retained. Now, let's imagine that the Voice becomes marked \"inactive\". At some point after the Voice becomes inactive, the Context will drop its reference and will stop rendering the Voice. The reference count of the Voice will drop to zero, and ARC will reclaim the memory for the voice. How do Voices become Inactive? A Voice is \"active\" as long as its _active instance variable is set to YES . A freshly initialized Voice is active. The _active instance variable may be set to NO by a number of ways. an external thread may directly mark a voice inactive by executing voice->_active = NO; This would cause the Voice to stop playing abruptly. a callback function in a Voice may mark itself inactive as part of the algorithm it implements In the Mclaren Synth Kit, Envelope Generators are a special type of Voice that mark themselves inactive after their decay time. Voices and Models A Voice is something that produces a buffer of Samples for a Context. A Model is an Objective-C object that holds properties that affect the behavior of a Voice. Models are designed to be updated in realtime by any number of control threads. GUI widgets and MIDI events should update Models, but not Voices directly. Voices are designed to read values from models on each period of the audio thread. Models may be allocated one-for-one for a voice, or can be shared among Voices. Models know how to save themselves to the NSDefaults system, and how to restore themselves as well. An Example The following example illustrates how to create a SIN-wave oscillator with an Envelope generator. We create two models: one for the envelope generator and one for an oscillator. The name given to each model becomes part of the key the model uses when it saves itself to the NSDefaults system. Next, an Envelope is created. This one is a \"linear envelope.\" The envelope is configured to read its realtime configuration parameters from envmodel1 . After that, a SIN-wave oscillator is created using the MSKGeneralOScillator class. This oscillator is given an initial note ( iNote ) value of middle-C (MIDI 60). Its realtime parameters input is set to the oscmodel1 model. In MSK, Voice properties that specify initial values begin with a lowercase 'i'. This oscillator also has an envelope input called sEnvelope . In MSK, Voice properties that begin with a lowercase 's'. The logic of the Oscillator Voice is defined such that it will mark itself \"inactive\" when the sEnvelope input becomes inactive. Adding the oscillator to the Context causes it to begin to sound. MSKEnvelopeModel *envmodel1 = [[MSKEnvelopeModel alloc] initWithName:@\"env1\"]; envmodel1.attack = 0.01; envmodel1.decay = 0.05; envmodel.sustain = 0.90; envmodel.rel = 2.00; MSKOscillatorModel *oscmodel1 = [[MSKOscillatorModel alloc] initWithName:@\"osc1\"]; oscmodel1.osctype = MSK_OSCILLATOR_TYPE_SIN; oscmodel1.pitchbend = 5; oscmodel.mod = 32; MSKExpEnvelope *env1 = [[MSKExpEnvelope alloc] initWithCtx:ctx]; env1.model = envmodel1; [env1 compile]; MSKGeneralOscillator *osc1 = [[MSKGeneralOscillator alloc] initWithCtx:ctx]; osc1.iNote = 60; osc1.sEnvelope = env; osc1.model = oscmodel1; [osc1 compile]; [ctx addVoice:osc1]; At this point, the system looks like the following figure. Variables env1 and osc hold pointers to the Envelope and the Oscillator, and the Context holds a pointer to the Oscillator. The Oscillator, in turn, holds a pointer to the Envelope. The Context looks to the Oscillator to produce new samples. The Oscillator, in turn, looks to the Envelope to produce new samples. The graph tracing from the Context to the Voices and Models defines the audio being rendered by the audio thread. noteOff and Decay Envelope Voices implement a formal protocol that is shown below. @protocol MSKContextEnvelope - (BOOL) noteOff; - (BOOL) noteAbort; - (BOOL) noteReset:(int)idx; @end Sending the noteOff method to an Envelope causes it to begin its release. After the release time, the Envelope is marked inactive. Sending the noteAbort method causes an Envelope to drop to 0.0 by the end of the period. The Envelope is then marked inactive. Sending noteReset causes an Envelope to start all over. In our example, we could send the noteOff method to the envelope to tell it to begin its release. We could also drop the references to the envelope and oscillator. [env1 noteOff]; env1 = nil; osc1 = nil; The resulting graph would be as shown below. The Context would still hold a reference to the Oscillator, and the Oscillator is still holding a reference to the Envelope. The Envelope, however, has begun its release. At the end of the release period, the envelope will mark itself \"inactive\" and then the oscillator will too. The Context will then release its reference to the oscillator and the entire graph will be reclaimed. Two Notes with Shared Models The example of this section will explain what happens as multiple notes are sounded with shared models. At some point in time, a thread executes the following causing the first new note to be added to the Context. Notice that we can forget the reference to osc1 , but that we still want to hold the reference to env1 so we can send it the noteOff method later. MSKExpEnvelope *env1 = [[MSKExpEnvelope alloc] initWithCtx:ctx]; env1.model = envmodel1; [env1 compile]; MSKGeneralOscillator *osc1 = [[MSKGeneralOscillator alloc] initWithCtx:ctx]; osc1.iNote = 60; osc1.sEnvelope = env; osc1.model = oscmodel1; [osc1 compile]; [ctx addVoice:osc1]; osc1 = nil; Some time after the first note, a second note is started by executing the following. We pass the oscillator to the context and hold a reference to the envelope. MSKExpEnvelope *env2 = [[MSKExpEnvelope alloc] initWithCtx:ctx]; env2.model = envmodel1; [env2 compile]; MSKGeneralOscillator *osc2 = [[MSKGeneralOscillator alloc] initWithCtx:ctx]; osc2.iNote = 64; osc2.sEnvelope = env; osc2.model = oscmodel1; [osc2 compile]; [ctx addVoice:osc2]; osc2 = nil; At this point, the Context is playing two notes: each is defined by a subgraph referenced by a pointer. The Voices of the notes are separate, but they do share Models. In this way, the two notes are using the same realtime parameters. Now, assume that it is time to release the first note. We do that by sending the noteOff message to its envelope. Since we don't need a reference to the envelope anymore, we set env1 to nil , and at this point the only reference to the subgraph of the first note is being held by the Context. [env1 noteOff]; env1 = nil; After the release time of the first note, the Envelope of the first note is marked \"inactive\" and it stops producing samples. Since the oscillator of the first note depends on the envelope, it too, is marked inactive. The Context drops the reference to the subgraph of the first note, and those Voices are reclaimed. Complex Note with Two Oscillators For our last example, we will describe a complex note built with one oscillator that modulates another. An MSKPhaseDistortionOscillator has an additional input that modulates the phase of each sample. This input may be any kind of Voice. In the example shown below there are two envelope generators that share the same model. In this way, these two envelopes will always share the same configuration parameters. The first envelope generator is a Linear Envelope generator and it sets the gain of the simple SIN-wave oscillator. The output of this entire first stage, rather than being rendered through the Context, is set as the input to the sPhaseDistortion input of a second oscillator. This second oscillator has its own envelope generator, but it is an Exponential Envelope generator. The output of the phase distortion oscillator is attached to the Context, so this is the sound that is rendered. This configuration shows an interesting situation: in order to instruct both envelope generators to begin their release, our code needs to hold pointers to both of the envelopes so that it can send the noteOff method. [env1 noteOff]; [env2 noteOff]; env1 = nil; env2 = nil; As before, once the envelope has completed its release phase, it is marked \"inactive\" and the oscillator depending on it is marked \"inactive\" as well. Then the Context releases its reference to the subgraph and the objects are reclaimed. The Compile Method The compile method is new in 2023, and it is required after connections to other objects are made and before adding the Voice to the Context for playing. The compile method examines the connections the Voice has made to other objects and optimizes the underlying sound generation code. For instance, if an Oscillator has an attached Envelope, an implementation using an Envelope is chosen. If the Oscillator has no attached envelope, then an implementation that does not read Envelope frames can be used, potentially speeding execution. MSKGeneralOscillator *osc = [[MSKGeneralOscillator alloc] initWithContext:ctx]; // ... configure the oscillator - parameters and connections to models and other voices [osc compile]; // your code is done connecting the oscillator to other objects Oscillators, Envelopes, Filters and Effects all require calling of the compile method. Summary This chapter provided an overview of the McLaren Synth Kit focusing on its two major classes: the MSKContext and the MSKVoice . A Context renders sounds by traversing a subgraph of Voices, each of which is a sample buffer with attached sound generator functions. The Context and the Voices cooperate to implement memory management and reclamation of Voices through a simple protocol that hides the details of how ARC is performed outside of the audio thread. This eliminates one source of error an audio program might create that could introduce audio artifacts. Sound design then becomes the construction of a graph and its traversal.","title":"McLaren Synth Kit - Overview"},{"location":"msk-over/#mclaren-synth-kit-overview","text":"In the previous chapter, we saw how to work with a PCM to deliver samples to an audio device. Working directly with an audio device exposes many of the hardware parameters to the underlying code. Parameters such as sample rate, period size, access pattern and sample type all must be accounted for. The Mclaren Synth Kit (M.S.K.) presents a level of abstraction above that of the PCM. It has been designed to use Objective-C to design audio generators as a connected graph of low-level audio units called Voices. This chapter describes how to set up and use the standard components of the Mclaren Synth Kit. The MSKContext class is the foundation of the Synth Kit. An MSKContext opens and manages a PCM. It schedules the necessary audio callback blocks and evaluates the voices of the audio graph to generate sounds. An MSKContext also allocates a dispatch queue for timing and for callbacks. As we learned earlier, it is important that the callbacks on the audio thread do not perform blocking operations. The MSKContext and its voices implement a protocol for getting information into and out-of the audio thread. By following the guidelines of this protocol you are ensured to avoid audio glitches or pops.","title":"McLaren Synth Kit - Overview"},{"location":"msk-over/#msk-context","text":"The MSKContext does its best to find good hardware and software parameters for the specified PCM device. Opening, configuring and running an MSKContext is straightfoward. The snippet below shows how to open the PCM device named \"default\" for playback. NSString *devName = @\"default\"; NSError *error; // Create an audio context on the 'default' device for playback MSKContext *ctx = [[MSKContext alloc] initWithName:devName andStream:SND_PCM_STREAM_PLAYBACK error:&error]; Next, create a configuration request. The request specifies the sample rate, period size and number of periods. These parameters are key for adjusting the latency of a synthesizer application. // Desired audio context parameters MSKContextRequest *request = [[MSKContextRequest alloc] init]; request.rate = 44000; request.persize = 1024; request.periods = 2; Now, configure the Context with the Request. BOOL ok; // Configure the context with the request ok = [ctx configureForRequest:request error:&error]; If there is no error, the Context has successfully configured the underlying PCM. You can observe the achieved Hardware and Software parameters in the hwparams and swparams property of the Context. NSLog(@\"%@\", ctx.hwparams) NSLog(@\"%@\", ctx.swparams) Aside: the Context exposes many of its internal variables as readonly properties. One of the design decisions behind the Mclaren Synth Kit is that useful implementation variables should be exposed rather than hidden in a private category. Before the Context produces any sound, it needs to be started. This call does two things: it starts the underlying PCM thread, and it launches a dispatch queue timer for monitoring status information coming out of the audio callback. // Start the context ok = [ctx startWithError:&error];","title":"MSK Context"},{"location":"msk-over/#msk-voice","text":"A Voice consists of a sample buffer and some callback functions that fill the buffer with samples. In the Mclaren Synth Kit there are voices that create sounds and voices that control the parameters of other voices. All are children of the MSKContextVoice base class. The compile method of a Voice must be called after it is attached to other objects. A Voice can be played by a Context by adding it to the Context with the addVoice: method. MSKContextVoice *v1 = [[MSKContextVoice alloc] initWithCtx:ctx]; [v1 compile]; [ctx addVoice:v1] It is useful to think about the audio graph constructed by the preceding code. A Voice referenced by the variable v1 has been added to the Context. The Context renders the Voice by reading its samples once every PCM period. The Context invokes a callback function defined by the Voice to create new samples each period. A Voice is rendered by a Context until it is no longer active. (We'll define how Voices become \"inactive\" in a minute.)","title":"MSK Voice"},{"location":"msk-over/#voices-and-automatic-reference-counting-arc","text":"A Voice is an Objective-C object and is managed through ARC (Automatic Reference Counting) just like any other object. A Context retains a reference to a Voice as long as it is active. When it is no longer active, the Context releases its reference so the Voice can be reclaimed. Consider the code below. This creates a Voice and hands it to the Context. After the addVoice: call, the variable v1 is set to nil , essentially dropping the reference to the Voice. MSKContextVoice *v1 = [[MSKContextVoice alloc] initWithCtx:ctx]; [v1 compile]; [ctx addVoice:v1] v1 = nil; The Context still holds a reference to the Voice, however. As long as it is \"active\" the reference is retained. Now, let's imagine that the Voice becomes marked \"inactive\". At some point after the Voice becomes inactive, the Context will drop its reference and will stop rendering the Voice. The reference count of the Voice will drop to zero, and ARC will reclaim the memory for the voice.","title":"Voices and Automatic Reference Counting (ARC)"},{"location":"msk-over/#how-do-voices-become-inactive","text":"A Voice is \"active\" as long as its _active instance variable is set to YES . A freshly initialized Voice is active. The _active instance variable may be set to NO by a number of ways. an external thread may directly mark a voice inactive by executing voice->_active = NO; This would cause the Voice to stop playing abruptly. a callback function in a Voice may mark itself inactive as part of the algorithm it implements In the Mclaren Synth Kit, Envelope Generators are a special type of Voice that mark themselves inactive after their decay time.","title":"How do Voices become Inactive?"},{"location":"msk-over/#voices-and-models","text":"A Voice is something that produces a buffer of Samples for a Context. A Model is an Objective-C object that holds properties that affect the behavior of a Voice. Models are designed to be updated in realtime by any number of control threads. GUI widgets and MIDI events should update Models, but not Voices directly. Voices are designed to read values from models on each period of the audio thread. Models may be allocated one-for-one for a voice, or can be shared among Voices. Models know how to save themselves to the NSDefaults system, and how to restore themselves as well.","title":"Voices and Models"},{"location":"msk-over/#an-example","text":"The following example illustrates how to create a SIN-wave oscillator with an Envelope generator. We create two models: one for the envelope generator and one for an oscillator. The name given to each model becomes part of the key the model uses when it saves itself to the NSDefaults system. Next, an Envelope is created. This one is a \"linear envelope.\" The envelope is configured to read its realtime configuration parameters from envmodel1 . After that, a SIN-wave oscillator is created using the MSKGeneralOScillator class. This oscillator is given an initial note ( iNote ) value of middle-C (MIDI 60). Its realtime parameters input is set to the oscmodel1 model. In MSK, Voice properties that specify initial values begin with a lowercase 'i'. This oscillator also has an envelope input called sEnvelope . In MSK, Voice properties that begin with a lowercase 's'. The logic of the Oscillator Voice is defined such that it will mark itself \"inactive\" when the sEnvelope input becomes inactive. Adding the oscillator to the Context causes it to begin to sound. MSKEnvelopeModel *envmodel1 = [[MSKEnvelopeModel alloc] initWithName:@\"env1\"]; envmodel1.attack = 0.01; envmodel1.decay = 0.05; envmodel.sustain = 0.90; envmodel.rel = 2.00; MSKOscillatorModel *oscmodel1 = [[MSKOscillatorModel alloc] initWithName:@\"osc1\"]; oscmodel1.osctype = MSK_OSCILLATOR_TYPE_SIN; oscmodel1.pitchbend = 5; oscmodel.mod = 32; MSKExpEnvelope *env1 = [[MSKExpEnvelope alloc] initWithCtx:ctx]; env1.model = envmodel1; [env1 compile]; MSKGeneralOscillator *osc1 = [[MSKGeneralOscillator alloc] initWithCtx:ctx]; osc1.iNote = 60; osc1.sEnvelope = env; osc1.model = oscmodel1; [osc1 compile]; [ctx addVoice:osc1]; At this point, the system looks like the following figure. Variables env1 and osc hold pointers to the Envelope and the Oscillator, and the Context holds a pointer to the Oscillator. The Oscillator, in turn, holds a pointer to the Envelope. The Context looks to the Oscillator to produce new samples. The Oscillator, in turn, looks to the Envelope to produce new samples. The graph tracing from the Context to the Voices and Models defines the audio being rendered by the audio thread.","title":"An Example"},{"location":"msk-over/#noteoff-and-decay","text":"Envelope Voices implement a formal protocol that is shown below. @protocol MSKContextEnvelope - (BOOL) noteOff; - (BOOL) noteAbort; - (BOOL) noteReset:(int)idx; @end Sending the noteOff method to an Envelope causes it to begin its release. After the release time, the Envelope is marked inactive. Sending the noteAbort method causes an Envelope to drop to 0.0 by the end of the period. The Envelope is then marked inactive. Sending noteReset causes an Envelope to start all over. In our example, we could send the noteOff method to the envelope to tell it to begin its release. We could also drop the references to the envelope and oscillator. [env1 noteOff]; env1 = nil; osc1 = nil; The resulting graph would be as shown below. The Context would still hold a reference to the Oscillator, and the Oscillator is still holding a reference to the Envelope. The Envelope, however, has begun its release. At the end of the release period, the envelope will mark itself \"inactive\" and then the oscillator will too. The Context will then release its reference to the oscillator and the entire graph will be reclaimed.","title":"noteOff and Decay"},{"location":"msk-over/#two-notes-with-shared-models","text":"The example of this section will explain what happens as multiple notes are sounded with shared models. At some point in time, a thread executes the following causing the first new note to be added to the Context. Notice that we can forget the reference to osc1 , but that we still want to hold the reference to env1 so we can send it the noteOff method later. MSKExpEnvelope *env1 = [[MSKExpEnvelope alloc] initWithCtx:ctx]; env1.model = envmodel1; [env1 compile]; MSKGeneralOscillator *osc1 = [[MSKGeneralOscillator alloc] initWithCtx:ctx]; osc1.iNote = 60; osc1.sEnvelope = env; osc1.model = oscmodel1; [osc1 compile]; [ctx addVoice:osc1]; osc1 = nil; Some time after the first note, a second note is started by executing the following. We pass the oscillator to the context and hold a reference to the envelope. MSKExpEnvelope *env2 = [[MSKExpEnvelope alloc] initWithCtx:ctx]; env2.model = envmodel1; [env2 compile]; MSKGeneralOscillator *osc2 = [[MSKGeneralOscillator alloc] initWithCtx:ctx]; osc2.iNote = 64; osc2.sEnvelope = env; osc2.model = oscmodel1; [osc2 compile]; [ctx addVoice:osc2]; osc2 = nil; At this point, the Context is playing two notes: each is defined by a subgraph referenced by a pointer. The Voices of the notes are separate, but they do share Models. In this way, the two notes are using the same realtime parameters. Now, assume that it is time to release the first note. We do that by sending the noteOff message to its envelope. Since we don't need a reference to the envelope anymore, we set env1 to nil , and at this point the only reference to the subgraph of the first note is being held by the Context. [env1 noteOff]; env1 = nil; After the release time of the first note, the Envelope of the first note is marked \"inactive\" and it stops producing samples. Since the oscillator of the first note depends on the envelope, it too, is marked inactive. The Context drops the reference to the subgraph of the first note, and those Voices are reclaimed.","title":"Two Notes with Shared Models"},{"location":"msk-over/#complex-note-with-two-oscillators","text":"For our last example, we will describe a complex note built with one oscillator that modulates another. An MSKPhaseDistortionOscillator has an additional input that modulates the phase of each sample. This input may be any kind of Voice. In the example shown below there are two envelope generators that share the same model. In this way, these two envelopes will always share the same configuration parameters. The first envelope generator is a Linear Envelope generator and it sets the gain of the simple SIN-wave oscillator. The output of this entire first stage, rather than being rendered through the Context, is set as the input to the sPhaseDistortion input of a second oscillator. This second oscillator has its own envelope generator, but it is an Exponential Envelope generator. The output of the phase distortion oscillator is attached to the Context, so this is the sound that is rendered. This configuration shows an interesting situation: in order to instruct both envelope generators to begin their release, our code needs to hold pointers to both of the envelopes so that it can send the noteOff method. [env1 noteOff]; [env2 noteOff]; env1 = nil; env2 = nil; As before, once the envelope has completed its release phase, it is marked \"inactive\" and the oscillator depending on it is marked \"inactive\" as well. Then the Context releases its reference to the subgraph and the objects are reclaimed.","title":"Complex Note with Two Oscillators"},{"location":"msk-over/#the-compile-method","text":"The compile method is new in 2023, and it is required after connections to other objects are made and before adding the Voice to the Context for playing. The compile method examines the connections the Voice has made to other objects and optimizes the underlying sound generation code. For instance, if an Oscillator has an attached Envelope, an implementation using an Envelope is chosen. If the Oscillator has no attached envelope, then an implementation that does not read Envelope frames can be used, potentially speeding execution. MSKGeneralOscillator *osc = [[MSKGeneralOscillator alloc] initWithContext:ctx]; // ... configure the oscillator - parameters and connections to models and other voices [osc compile]; // your code is done connecting the oscillator to other objects Oscillators, Envelopes, Filters and Effects all require calling of the compile method.","title":"The Compile Method"},{"location":"msk-over/#summary","text":"This chapter provided an overview of the McLaren Synth Kit focusing on its two major classes: the MSKContext and the MSKVoice . A Context renders sounds by traversing a subgraph of Voices, each of which is a sample buffer with attached sound generator functions. The Context and the Voices cooperate to implement memory management and reclamation of Voices through a simple protocol that hides the details of how ARC is performed outside of the audio thread. This eliminates one source of error an audio program might create that could introduce audio artifacts. Sound design then becomes the construction of a graph and its traversal.","title":"Summary"},{"location":"setup/","text":"Setting up your Environment This chapter gives some pointers about how to set up a GNUstep development environment on Linux. There are two good alternatives. Both install the clang compiler and build GNUstep from source. install gs-desktop install just GNUstep libraries Install gs-desktop There is a project that resurrects the entire GNUstep experience including development environment, terminals, desktop and utilites. It is called https://github.com/onflapp/gs-desktop . This is an active project and here at McLaren Labs we are really liking it. To use it, create an OS image with just a minimal installation (we are using Debian 12) and follow the directions. One really nice feature of the installation is that you can select which desktop you want to use from the login screen. Thus, you can keep a GNOME based desktop and a GNUstep desktop in the same OS and choose which one to use at login. Install GNUstep If you don't want to install an entire desktop environment, you can still use GNUstep libraries on their own. The easiest way to install and build GNUstep is with scripts in the following repository: https://github.com/plaurent/gnustep-build/ These scripts automate the entire process of first installing the dependencies of the GNUstep environment, downloading the necessary repositories from Github and building everything. First, download the build repo. $ mkdir ~/git $ cd ~/git $ git clone https://github.com/plaurent/gnustep-build Then, make a new directory to perform the build in. $ cd ~ $ mkdir gbuild In the directory you just made, run the build command corresponding to your Operating System. $ cd gbuild $ ~/git/gnustep-build/ubuntu-20.10-clang-11.0-runtime-2.0/GNUstep-buildon-ubuntu2010.sh After about 20 minutes on a fast laptop with a good connection, you'll have a complete GNUstep development environment. The installation script writes a modification to your .bashrc so that new terminals get the proper environment variables configured. Open up a new `xterm and see if it's installed. $ clang -v $ gnustep-config --help A Minimal ObjC program If you're new to Objective-C on Linux, try compiling this tiny program to see if everything is working correctly. #import <Foundation/Foundation.h> #import <dispatch/dispatch.h> int main(int argc, char *argv[], char **env) { NSLog(@\"Hello World!\\n\"); NSString *greeting = @\"Hello from a dispatch event\"; dispatch_time_t delay = dispatch_time(DISPATCH_TIME_NOW, 2.0*NSEC_PER_SEC); dispatch_after(delay, dispatch_get_global_queue(0,0), ^(void) { NSLog(@\"dispatch: %@\", greeting); exit(0); }); [[NSRunLoop mainRunLoop] run]; } Then, to compile it use gnustep-config to pick up all of the flags the compiler needs for GNUstep base libraries. But note, we still have to specify libdispatch explicitly. $ clang -o objctest1 `gnustep-config --objc-flags` `gnustep-config --base-libs` objctest1.m -ldispatch This program is available in the ../examples-setup directory along with a Makefile.","title":"GNUstep Setup"},{"location":"setup/#setting-up-your-environment","text":"This chapter gives some pointers about how to set up a GNUstep development environment on Linux. There are two good alternatives. Both install the clang compiler and build GNUstep from source. install gs-desktop install just GNUstep libraries","title":"Setting up your Environment"},{"location":"setup/#install-gs-desktop","text":"There is a project that resurrects the entire GNUstep experience including development environment, terminals, desktop and utilites. It is called https://github.com/onflapp/gs-desktop . This is an active project and here at McLaren Labs we are really liking it. To use it, create an OS image with just a minimal installation (we are using Debian 12) and follow the directions. One really nice feature of the installation is that you can select which desktop you want to use from the login screen. Thus, you can keep a GNOME based desktop and a GNUstep desktop in the same OS and choose which one to use at login.","title":"Install gs-desktop"},{"location":"setup/#install-gnustep","text":"If you don't want to install an entire desktop environment, you can still use GNUstep libraries on their own. The easiest way to install and build GNUstep is with scripts in the following repository: https://github.com/plaurent/gnustep-build/ These scripts automate the entire process of first installing the dependencies of the GNUstep environment, downloading the necessary repositories from Github and building everything. First, download the build repo. $ mkdir ~/git $ cd ~/git $ git clone https://github.com/plaurent/gnustep-build Then, make a new directory to perform the build in. $ cd ~ $ mkdir gbuild In the directory you just made, run the build command corresponding to your Operating System. $ cd gbuild $ ~/git/gnustep-build/ubuntu-20.10-clang-11.0-runtime-2.0/GNUstep-buildon-ubuntu2010.sh After about 20 minutes on a fast laptop with a good connection, you'll have a complete GNUstep development environment. The installation script writes a modification to your .bashrc so that new terminals get the proper environment variables configured. Open up a new `xterm and see if it's installed. $ clang -v $ gnustep-config --help","title":"Install GNUstep"},{"location":"setup/#a-minimal-objc-program","text":"If you're new to Objective-C on Linux, try compiling this tiny program to see if everything is working correctly. #import <Foundation/Foundation.h> #import <dispatch/dispatch.h> int main(int argc, char *argv[], char **env) { NSLog(@\"Hello World!\\n\"); NSString *greeting = @\"Hello from a dispatch event\"; dispatch_time_t delay = dispatch_time(DISPATCH_TIME_NOW, 2.0*NSEC_PER_SEC); dispatch_after(delay, dispatch_get_global_queue(0,0), ^(void) { NSLog(@\"dispatch: %@\", greeting); exit(0); }); [[NSRunLoop mainRunLoop] run]; } Then, to compile it use gnustep-config to pick up all of the flags the compiler needs for GNUstep base libraries. But note, we still have to specify libdispatch explicitly. $ clang -o objctest1 `gnustep-config --objc-flags` `gnustep-config --base-libs` objctest1.m -ldispatch This program is available in the ../examples-setup directory along with a Makefile.","title":"A Minimal ObjC program"},{"location":"toc/","text":"Table Of Contents Introduction : Overview of the goals of the McLaren Synth Kit and its group of Frameworks. GNUstep and VSCode : How to great a GNUStep environment on your Linux computer. Also, a \"how-to\" about using VSCode with Objective-C on Linux for a nice development environment. Alsa Sound Kit - MIDI : An introduction to the MIDI event and route handling capabilities provided by the Alsa Sound Kit. Alsa Sound Kit - PCM : An introduction to the PCM (audio device) handling provided by the Alsa Sound Kit. McLaren Synth Kit - Overview : An explanation of the features of the McLaren Synth Kit. It introduces the MSKContext which abstracts an audio device, and the powerful audio operators that help in constructing sounds. Conclusion : Some concluding remarks about the McLaren Synth Kit.","title":"Table Of Contents"},{"location":"toc/#table-of-contents","text":"Introduction : Overview of the goals of the McLaren Synth Kit and its group of Frameworks. GNUstep and VSCode : How to great a GNUStep environment on your Linux computer. Also, a \"how-to\" about using VSCode with Objective-C on Linux for a nice development environment. Alsa Sound Kit - MIDI : An introduction to the MIDI event and route handling capabilities provided by the Alsa Sound Kit. Alsa Sound Kit - PCM : An introduction to the PCM (audio device) handling provided by the Alsa Sound Kit. McLaren Synth Kit - Overview : An explanation of the features of the McLaren Synth Kit. It introduces the MSKContext which abstracts an audio device, and the powerful audio operators that help in constructing sounds. Conclusion : Some concluding remarks about the McLaren Synth Kit.","title":"Table Of Contents"}]}